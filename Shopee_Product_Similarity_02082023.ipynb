{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-11T17:17:59.952422Z",
     "iopub.status.busy": "2021-05-11T17:17:59.951728Z",
     "iopub.status.idle": "2021-05-11T17:18:55.331927Z",
     "shell.execute_reply": "2021-05-11T17:18:55.331325Z"
    },
    "papermill": {
     "duration": 55.404189,
     "end_time": "2021-05-11T17:18:55.332116",
     "exception": false,
     "start_time": "2021-05-11T17:17:59.927927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl\n",
    "!pip install ../input/shopee-libs/editdistance-0.5.3-cp37-cp37m-manylinux1_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:18:55.368169Z",
     "iopub.status.busy": "2021-05-11T17:18:55.367627Z",
     "iopub.status.idle": "2021-05-11T17:18:55.373611Z",
     "shell.execute_reply": "2021-05-11T17:18:55.373208Z"
    },
    "papermill": {
     "duration": 0.025704,
     "end_time": "2021-05-11T17:18:55.373728",
     "exception": false,
     "start_time": "2021-05-11T17:18:55.348024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lyk_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lyk_config.py\n",
    "\n",
    "k = 50\n",
    "conf_th = 0.7\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DEBUG = len(pd.read_csv('../input/shopee-product-matching/test.csv')) == 3\n",
    "\n",
    "def load_data():\n",
    "    if DEBUG:\n",
    "        nrows = 1000\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv', nrows=nrows, usecols=['posting_id', 'image', 'title'])\n",
    "#         nrows = None\n",
    "#         df = pd.read_csv('../input/shopee-product-matching/train.csv', nrows=nrows, usecols=['posting_id', 'image', 'title']).append(\n",
    "#              pd.read_csv('../input/shopee-product-matching/train.csv', nrows=nrows, usecols=['posting_id', 'image', 'title'])).reset_index(drop=True)\n",
    "        img_dir = Path('../input/shopee-product-matching/train_images/')\n",
    "    else:\n",
    "        nrows = None\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv', usecols=['posting_id', 'image', 'title'])\n",
    "        img_dir = Path('../input/shopee-product-matching/test_images/')\n",
    "    return df, img_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01524,
     "end_time": "2021-05-11T17:18:55.404705",
     "exception": false,
     "start_time": "2021-05-11T17:18:55.389465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image similarity, Multi-modal similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:18:55.445534Z",
     "iopub.status.busy": "2021-05-11T17:18:55.441677Z",
     "iopub.status.idle": "2021-05-11T17:20:53.425224Z",
     "shell.execute_reply": "2021-05-11T17:20:53.425641Z"
    },
    "papermill": {
     "duration": 118.005377,
     "end_time": "2021-05-11T17:20:53.425801",
     "exception": false,
     "start_time": "2021-05-11T17:18:55.420424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"E:\\MSc DataAnalytics\\Learning\\Semester3\\Thesis\\Code\\lyk_config.py\", line 8, in <module>\n",
      "    DEBUG = len(pd.read_csv('../input/shopee-product-matching/test.csv')) == 3\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 680, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 575, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 933, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1217, in _make_engine\n",
      "    self.handles = get_handle(  # type: ignore[call-overload]\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\", line 789, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../input/shopee-product-matching/test.csv'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"from lyk_config import k, conf_th, DEBUG, load_data\\n\\nimport sys\\nsys.path.append('../input/timm045/')\\nimport timm\\n\\nfrom itertools import zip_longest\\nimport json\\nimport math\\nimport gc\\nimport os\\nfrom pathlib import Path\\n\\nimport faiss\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom torchvision.io import read_image\\nfrom torchvision.transforms import Resize, RandomHorizontalFlip, ColorJitter, Normalize, Compose, RandomResizedCrop, CenterCrop, ToTensor\\n\\nfrom tqdm import tqdm\\nfrom PIL import Image\\nimport joblib\\nfrom scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\\nimport editdistance\\nimport networkx as nx\\nfrom transformers import BertConfig, BertModel, BertTokenizerFast\\n\\nNUM_CLASSES = 11014\\nNUM_WORKERS = 2\\nSEED = 0\\n\\n\\ndef gem(x, p=3, eps=1e-6):\\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\\n\\n    \\nclass ShopeeNet(nn.Module):\\n\\n    def __init__(self,\\n                 backbone,\\n                 num_classes,\\n                 fc_dim=512,\\n                 s=30, margin=0.5, p=3):\\n        super(ShopeeNet, self).__init__()\\n\\n        self.backbone = backbone\\n        self.backbone.reset_classifier(num_classes=0)  # remove classifier\\n\\n        self.fc = nn.Linear(self.backbone.num_features, fc_dim)\\n        self.bn = nn.BatchNorm1d(fc_dim)\\n        self._init_params()\\n        self.p = p\\n\\n    def _init_params(self):\\n        nn.init.xavier_normal_(self.fc.weight)\\n        nn.init.constant_(self.fc.bias, 0)\\n        nn.init.constant_(self.bn.weight, 1)\\n        nn.init.constant_(self.bn.bias, 0)\\n\\n    def extract_feat(self, x):\\n        batch_size = x.shape[0]\\n        x = self.backbone.forward_features(x)\\n        if isinstance(x, tuple):\\n            x = (x[0] + x[1]) / 2\\n            x = self.bn(x)\\n        else:\\n            x = gem(x, p=self.p).view(batch_size, -1)\\n            x = self.fc(x)\\n            x = self.bn(x)\\n        return x\\n\\n    def forward(self, x, label):\\n        feat = self.extract_feat(x)\\n        x = self.loss_module(feat, label)\\n        return x, feat\\n\\n\\nclass ShopeeDataset(Dataset):\\n\\n    def __init__(self, df, img_dir, transform=None):\\n        self.df = df\\n        self.img_dir = img_dir\\n        self.transform = transform\\n\\n    def __getitem__(self, index):\\n        row = self.df.iloc[index]\\n        img = read_image(str(self.img_dir / row['image']))\\n        _, h, w = img.shape\\n        st_size = (self.img_dir / row['image']).stat().st_size\\n        if self.transform is not None:\\n            img = self.transform(img)\\n\\n        return img, row['title'], h, w, st_size\\n\\n    def __len__(self):\\n        return len(self.df)\\n\\n\\nclass MultiModalNet(nn.Module):\\n\\n    def __init__(self,\\n                 backbone,\\n                 bert_model,\\n                 num_classes,\\n                 tokenizer,\\n                 max_len=32,\\n                 fc_dim=512,\\n                 s=30, margin=0.5, p=3, loss='ArcMarginProduct'):\\n        super().__init__()\\n\\n        self.backbone = backbone\\n        self.backbone.reset_classifier(num_classes=0)  # remove classifier\\n\\n        self.bert_model = bert_model\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.fc = nn.Linear(self.bert_model.config.hidden_size + self.backbone.num_features, fc_dim)\\n        self.bn = nn.BatchNorm1d(fc_dim)\\n        self._init_params()\\n        self.p = p\\n\\n    def _init_params(self):\\n        nn.init.xavier_normal_(self.fc.weight)\\n        nn.init.constant_(self.fc.bias, 0)\\n        nn.init.constant_(self.bn.weight, 1)\\n        nn.init.constant_(self.bn.bias, 0)\\n\\n    def extract_feat(self, img, title):\\n        batch_size = img.shape[0]\\n        img = self.backbone.forward_features(img)\\n        img = gem(img, p=self.p).view(batch_size, -1)\\n\\n        tokenizer_output = self.tokenizer(title, truncation=True, padding=True, max_length=self.max_len)\\n        input_ids = torch.LongTensor(tokenizer_output['input_ids']).to('cuda')\\n        token_type_ids = torch.LongTensor(tokenizer_output['token_type_ids']).to('cuda')\\n        attention_mask = torch.LongTensor(tokenizer_output['attention_mask']).to('cuda')\\n        title = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\\n        # x = x.last_hidden_state.sum(dim=1) / attention_mask.sum(dim=1, keepdims=True)\\n        title = title.last_hidden_state.mean(dim=1)\\n\\n        x = torch.cat([img, title], dim=1)\\n        x = self.fc(x)\\n        x = self.bn(x)\\n        return x\\n\\n\\n####\\n\\ndf, img_dir = load_data()\\n    \\n###\\n\\ncheckpoint1 = torch.load('../input/shopee/v45.pth')\\ncheckpoint2 = torch.load('../input/shopee/v34.pth')\\ncheckpoint3 = torch.load('../input/shopee/v79.pth')\\nparams1 = checkpoint1['params']\\nparams2 = checkpoint2['params']\\nparams3 = checkpoint3['params']\\n\\ntransform = Compose([\\n    Resize(size=params1['test_size'] + 32, interpolation=Image.BICUBIC),\\n    CenterCrop((params1['test_size'], params1['test_size'])),\\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n])\\ndataset = ShopeeDataset(df=df, img_dir=img_dir, transform=None)\\ndata_loader = DataLoader(dataset, batch_size=8, shuffle=False,\\n                         drop_last=False, pin_memory=True, num_workers=NUM_WORKERS, collate_fn=lambda x: x)\\n\\nbackbone = timm.create_model(model_name=params1['backbone'], pretrained=False)\\nmodel1 = ShopeeNet(backbone, num_classes=0, fc_dim=params1['fc_dim'])\\nmodel1 = model1.to('cuda')\\nmodel1.load_state_dict(checkpoint1['model'], strict=False)\\nmodel1.train(False)\\nmodel1.p = params1['p_eval']\\n\\nbackbone = timm.create_model(model_name=params2['backbone'], pretrained=False)\\nmodel2 = ShopeeNet(backbone, num_classes=0, fc_dim=params2['fc_dim'])\\nmodel2 = model2.to('cuda')\\nmodel2.load_state_dict(checkpoint2['model'], strict=False)\\nmodel2.train(False)\\nmodel2.p = params2['p_eval']\\n\\nbackbone = timm.create_model(model_name=params3['backbone'], pretrained=False)\\ntokenizer = BertTokenizerFast(vocab_file='../input/bert-indo/vocab.txt')\\nbert_config = BertConfig.from_json_file('../input/bert-indo/config.json')\\nbert_model = BertModel(bert_config)\\nmodel3 = MultiModalNet(backbone, bert_model, num_classes=0, tokenizer=tokenizer, max_len=params3['max_len'],\\n                       fc_dim=params3['fc_dim'], s=params3['s'], margin=params3['margin'], loss=params3['loss'])\\nmodel3 = model3.to('cuda')\\nmodel3.load_state_dict(checkpoint3['model'], strict=False)\\nmodel3.train(False)\\nmodel3.p = params3['p_eval']\\n\\nimg_feats1 = []\\nimg_feats2 = []\\nmm_feats = []\\nimg_hs = []\\nimg_ws = []\\nst_sizes = []\\nfor batch in tqdm(data_loader, total=len(data_loader), miniters=None, ncols=55):\\n    img, title, h, w, st_size = list(zip(*batch))\\n    img = torch.cat([transform(x.to('cuda').float() / 255)[None] for x in img], axis=0)\\n    title = list(title)\\n    with torch.no_grad():\\n        feats_minibatch1 = model1.extract_feat(img)\\n        img_feats1.append(feats_minibatch1.cpu().numpy())\\n        feats_minibatch2 = model2.extract_feat(img)\\n        img_feats2.append(feats_minibatch2.cpu().numpy())\\n        feats_minibatch3 = model3.extract_feat(img, title)\\n        mm_feats.append(feats_minibatch3.cpu().numpy())\\n    img_hs.extend(list(h))\\n    img_ws.extend(list(w))\\n    st_sizes.extend(list(st_size))\\n\\nimg_feats1 = np.concatenate(img_feats1)\\nimg_feats1 /= np.linalg.norm(img_feats1, 2, axis=1, keepdims=True)\\nimg_feats2 = np.concatenate(img_feats2)\\nimg_feats2 /= np.linalg.norm(img_feats2, 2, axis=1, keepdims=True)\\nmm_feats = np.concatenate(mm_feats)\\nmm_feats /= np.linalg.norm(mm_feats, 2, axis=1, keepdims=True)\\n\\nnp.save('/tmp/img_feats1', img_feats1)\\nnp.save('/tmp/img_feats2', img_feats2)\\n\\nimg_feats = np.concatenate([\\n    img_feats1 * 1.0,\\n    img_feats2 * 1.0,\\n], axis=1)\\nimg_feats /= np.linalg.norm(img_feats, 2, axis=1, keepdims=True)\\n###\\n\\nnp.save('/tmp/img_feats', img_feats)\\n\\nres = faiss.StandardGpuResources()\\nindex_img = faiss.IndexFlatIP(params1['fc_dim'] + params2['fc_dim'])\\nindex_img = faiss.index_cpu_to_gpu(res, 0, index_img)\\nindex_img.add(img_feats)\\nsimilarities_img, indexes_img = index_img.search(img_feats, k)\\n\\n\\njoblib.dump([similarities_img, indexes_img], '/tmp/lyk_img_data.pkl')\\njoblib.dump([st_sizes, img_hs, img_ws], '/tmp/lyk_img_meta_data.pkl')\\n\\nres = faiss.StandardGpuResources()\\nindex_mm = faiss.IndexFlatIP(params3['fc_dim'])\\nindex_mm = faiss.index_cpu_to_gpu(res, 0, index_mm)\\nindex_mm.add(mm_feats)\\nsimilarities_mm, indexes_mm = index_mm.search(mm_feats, k)\\n\\njoblib.dump([similarities_mm, indexes_mm], '/tmp/lyk_mm_data.pkl')\\n\\n### for TKM\\nnp.save('/tmp/mm_feats', mm_feats)\\n\"' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrom lyk_config import k, conf_th, DEBUG, load_data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport sys\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43msys.path.append(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/timm045/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport timm\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom itertools import zip_longest\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport json\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport math\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport gc\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport os\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom pathlib import Path\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport faiss\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport numpy as np\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch.nn as nn\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch.nn.functional as F\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch.optim as optim\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torch.utils.data import DataLoader, Dataset\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torchvision.io import read_image\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom torchvision.transforms import Resize, RandomHorizontalFlip, ColorJitter, Normalize, Compose, RandomResizedCrop, CenterCrop, ToTensor\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom tqdm import tqdm\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom PIL import Image\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport joblib\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport editdistance\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport networkx as nx\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom transformers import BertConfig, BertModel, BertTokenizerFast\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNUM_CLASSES = 11014\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNUM_WORKERS = 2\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSEED = 0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef gem(x, p=3, eps=1e-6):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass ShopeeNet(nn.Module):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __init__(self,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 backbone,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 num_classes,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 fc_dim=512,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 s=30, margin=0.5, p=3):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        super(ShopeeNet, self).__init__()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.backbone = backbone\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.backbone.reset_classifier(num_classes=0)  # remove classifier\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.fc = nn.Linear(self.backbone.num_features, fc_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.bn = nn.BatchNorm1d(fc_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self._init_params()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.p = p\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def _init_params(self):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.xavier_normal_(self.fc.weight)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.constant_(self.fc.bias, 0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.constant_(self.bn.weight, 1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.constant_(self.bn.bias, 0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def extract_feat(self, x):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        batch_size = x.shape[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        x = self.backbone.forward_features(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if isinstance(x, tuple):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            x = (x[0] + x[1]) / 2\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            x = self.bn(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            x = gem(x, p=self.p).view(batch_size, -1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            x = self.fc(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            x = self.bn(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return x\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def forward(self, x, label):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        feat = self.extract_feat(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        x = self.loss_module(feat, label)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return x, feat\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass ShopeeDataset(Dataset):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __init__(self, df, img_dir, transform=None):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.df = df\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.img_dir = img_dir\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.transform = transform\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __getitem__(self, index):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        row = self.df.iloc[index]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        img = read_image(str(self.img_dir / row[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        _, h, w = img.shape\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        st_size = (self.img_dir / row[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]).stat().st_size\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        if self.transform is not None:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            img = self.transform(img)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return img, row[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], h, w, st_size\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __len__(self):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return len(self.df)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass MultiModalNet(nn.Module):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def __init__(self,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 backbone,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 bert_model,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 num_classes,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 tokenizer,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 max_len=32,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 fc_dim=512,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                 s=30, margin=0.5, p=3, loss=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArcMarginProduct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        super().__init__()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.backbone = backbone\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.backbone.reset_classifier(num_classes=0)  # remove classifier\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.bert_model = bert_model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.tokenizer = tokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.max_len = max_len\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.fc = nn.Linear(self.bert_model.config.hidden_size + self.backbone.num_features, fc_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.bn = nn.BatchNorm1d(fc_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self._init_params()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        self.p = p\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def _init_params(self):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.xavier_normal_(self.fc.weight)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.constant_(self.fc.bias, 0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.constant_(self.bn.weight, 1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        nn.init.constant_(self.bn.bias, 0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def extract_feat(self, img, title):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        batch_size = img.shape[0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        img = self.backbone.forward_features(img)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        img = gem(img, p=self.p).view(batch_size, -1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        tokenizer_output = self.tokenizer(title, truncation=True, padding=True, max_length=self.max_len)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        input_ids = torch.LongTensor(tokenizer_output[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]).to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        token_type_ids = torch.LongTensor(tokenizer_output[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtoken_type_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]).to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        attention_mask = torch.LongTensor(tokenizer_output[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]).to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        title = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        # x = x.last_hidden_state.sum(dim=1) / attention_mask.sum(dim=1, keepdims=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        title = title.last_hidden_state.mean(dim=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        x = torch.cat([img, title], dim=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        x = self.fc(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        x = self.bn(x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        return x\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m####\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdf, img_dir = load_data()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m###\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint1 = torch.load(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/shopee/v45.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint2 = torch.load(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/shopee/v34.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint3 = torch.load(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/shopee/v79.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mparams1 = checkpoint1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mparams2 = checkpoint2[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mparams3 = checkpoint3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtransform = Compose([\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    Resize(size=params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m] + 32, interpolation=Image.BICUBIC),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    CenterCrop((params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m])),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset = ShopeeDataset(df=df, img_dir=img_dir, transform=None)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdata_loader = DataLoader(dataset, batch_size=8, shuffle=False,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                         drop_last=False, pin_memory=True, num_workers=NUM_WORKERS, collate_fn=lambda x: x)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbackbone = timm.create_model(model_name=params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackbone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], pretrained=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel1 = ShopeeNet(backbone, num_classes=0, fc_dim=params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel1 = model1.to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel1.load_state_dict(checkpoint1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], strict=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel1.train(False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel1.p = params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp_eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbackbone = timm.create_model(model_name=params2[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackbone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], pretrained=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel2 = ShopeeNet(backbone, num_classes=0, fc_dim=params2[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel2 = model2.to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel2.load_state_dict(checkpoint2[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], strict=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel2.train(False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel2.p = params2[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp_eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbackbone = timm.create_model(model_name=params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackbone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], pretrained=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtokenizer = BertTokenizerFast(vocab_file=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/bert-indo/vocab.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbert_config = BertConfig.from_json_file(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/bert-indo/config.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbert_model = BertModel(bert_config)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel3 = MultiModalNet(backbone, bert_model, num_classes=0, tokenizer=tokenizer, max_len=params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                       fc_dim=params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], s=params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], margin=params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], loss=params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel3 = model3.to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel3.load_state_dict(checkpoint3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m], strict=False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel3.train(False)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel3.p = params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp_eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats1 = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats2 = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmm_feats = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_hs = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_ws = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mst_sizes = []\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfor batch in tqdm(data_loader, total=len(data_loader), miniters=None, ncols=55):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    img, title, h, w, st_size = list(zip(*batch))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    img = torch.cat([transform(x.to(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m).float() / 255)[None] for x in img], axis=0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    title = list(title)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    with torch.no_grad():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        feats_minibatch1 = model1.extract_feat(img)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        img_feats1.append(feats_minibatch1.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        feats_minibatch2 = model2.extract_feat(img)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        img_feats2.append(feats_minibatch2.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        feats_minibatch3 = model3.extract_feat(img, title)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        mm_feats.append(feats_minibatch3.cpu().numpy())\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    img_hs.extend(list(h))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    img_ws.extend(list(w))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    st_sizes.extend(list(st_size))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats1 = np.concatenate(img_feats1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats1 /= np.linalg.norm(img_feats1, 2, axis=1, keepdims=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats2 = np.concatenate(img_feats2)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats2 /= np.linalg.norm(img_feats2, 2, axis=1, keepdims=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmm_feats = np.concatenate(mm_feats)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmm_feats /= np.linalg.norm(mm_feats, 2, axis=1, keepdims=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnp.save(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/img_feats1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, img_feats1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnp.save(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/img_feats2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, img_feats2)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats = np.concatenate([\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    img_feats1 * 1.0,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    img_feats2 * 1.0,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m], axis=1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimg_feats /= np.linalg.norm(img_feats, 2, axis=1, keepdims=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m###\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnp.save(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/img_feats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, img_feats)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mres = faiss.StandardGpuResources()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mindex_img = faiss.IndexFlatIP(params1[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m] + params2[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mindex_img = faiss.index_cpu_to_gpu(res, 0, index_img)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mindex_img.add(img_feats)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43msimilarities_img, indexes_img = index_img.search(img_feats, k)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mjoblib.dump([similarities_img, indexes_img], \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/lyk_img_data.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mjoblib.dump([st_sizes, img_hs, img_ws], \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/lyk_img_meta_data.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mres = faiss.StandardGpuResources()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mindex_mm = faiss.IndexFlatIP(params3[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mindex_mm = faiss.index_cpu_to_gpu(res, 0, index_mm)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mindex_mm.add(mm_feats)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43msimilarities_mm, indexes_mm = index_mm.search(mm_feats, k)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mjoblib.dump([similarities_mm, indexes_mm], \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/lyk_mm_data.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m### for TKM\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnp.save(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/mm_feats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, mm_feats)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2347\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2346\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2347\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[1;34m(line, cell)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[0;32m    304\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'b\"from lyk_config import k, conf_th, DEBUG, load_data\\n\\nimport sys\\nsys.path.append('../input/timm045/')\\nimport timm\\n\\nfrom itertools import zip_longest\\nimport json\\nimport math\\nimport gc\\nimport os\\nfrom pathlib import Path\\n\\nimport faiss\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom torchvision.io import read_image\\nfrom torchvision.transforms import Resize, RandomHorizontalFlip, ColorJitter, Normalize, Compose, RandomResizedCrop, CenterCrop, ToTensor\\n\\nfrom tqdm import tqdm\\nfrom PIL import Image\\nimport joblib\\nfrom scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\\nimport editdistance\\nimport networkx as nx\\nfrom transformers import BertConfig, BertModel, BertTokenizerFast\\n\\nNUM_CLASSES = 11014\\nNUM_WORKERS = 2\\nSEED = 0\\n\\n\\ndef gem(x, p=3, eps=1e-6):\\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\\n\\n    \\nclass ShopeeNet(nn.Module):\\n\\n    def __init__(self,\\n                 backbone,\\n                 num_classes,\\n                 fc_dim=512,\\n                 s=30, margin=0.5, p=3):\\n        super(ShopeeNet, self).__init__()\\n\\n        self.backbone = backbone\\n        self.backbone.reset_classifier(num_classes=0)  # remove classifier\\n\\n        self.fc = nn.Linear(self.backbone.num_features, fc_dim)\\n        self.bn = nn.BatchNorm1d(fc_dim)\\n        self._init_params()\\n        self.p = p\\n\\n    def _init_params(self):\\n        nn.init.xavier_normal_(self.fc.weight)\\n        nn.init.constant_(self.fc.bias, 0)\\n        nn.init.constant_(self.bn.weight, 1)\\n        nn.init.constant_(self.bn.bias, 0)\\n\\n    def extract_feat(self, x):\\n        batch_size = x.shape[0]\\n        x = self.backbone.forward_features(x)\\n        if isinstance(x, tuple):\\n            x = (x[0] + x[1]) / 2\\n            x = self.bn(x)\\n        else:\\n            x = gem(x, p=self.p).view(batch_size, -1)\\n            x = self.fc(x)\\n            x = self.bn(x)\\n        return x\\n\\n    def forward(self, x, label):\\n        feat = self.extract_feat(x)\\n        x = self.loss_module(feat, label)\\n        return x, feat\\n\\n\\nclass ShopeeDataset(Dataset):\\n\\n    def __init__(self, df, img_dir, transform=None):\\n        self.df = df\\n        self.img_dir = img_dir\\n        self.transform = transform\\n\\n    def __getitem__(self, index):\\n        row = self.df.iloc[index]\\n        img = read_image(str(self.img_dir / row['image']))\\n        _, h, w = img.shape\\n        st_size = (self.img_dir / row['image']).stat().st_size\\n        if self.transform is not None:\\n            img = self.transform(img)\\n\\n        return img, row['title'], h, w, st_size\\n\\n    def __len__(self):\\n        return len(self.df)\\n\\n\\nclass MultiModalNet(nn.Module):\\n\\n    def __init__(self,\\n                 backbone,\\n                 bert_model,\\n                 num_classes,\\n                 tokenizer,\\n                 max_len=32,\\n                 fc_dim=512,\\n                 s=30, margin=0.5, p=3, loss='ArcMarginProduct'):\\n        super().__init__()\\n\\n        self.backbone = backbone\\n        self.backbone.reset_classifier(num_classes=0)  # remove classifier\\n\\n        self.bert_model = bert_model\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n        self.fc = nn.Linear(self.bert_model.config.hidden_size + self.backbone.num_features, fc_dim)\\n        self.bn = nn.BatchNorm1d(fc_dim)\\n        self._init_params()\\n        self.p = p\\n\\n    def _init_params(self):\\n        nn.init.xavier_normal_(self.fc.weight)\\n        nn.init.constant_(self.fc.bias, 0)\\n        nn.init.constant_(self.bn.weight, 1)\\n        nn.init.constant_(self.bn.bias, 0)\\n\\n    def extract_feat(self, img, title):\\n        batch_size = img.shape[0]\\n        img = self.backbone.forward_features(img)\\n        img = gem(img, p=self.p).view(batch_size, -1)\\n\\n        tokenizer_output = self.tokenizer(title, truncation=True, padding=True, max_length=self.max_len)\\n        input_ids = torch.LongTensor(tokenizer_output['input_ids']).to('cuda')\\n        token_type_ids = torch.LongTensor(tokenizer_output['token_type_ids']).to('cuda')\\n        attention_mask = torch.LongTensor(tokenizer_output['attention_mask']).to('cuda')\\n        title = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\\n        # x = x.last_hidden_state.sum(dim=1) / attention_mask.sum(dim=1, keepdims=True)\\n        title = title.last_hidden_state.mean(dim=1)\\n\\n        x = torch.cat([img, title], dim=1)\\n        x = self.fc(x)\\n        x = self.bn(x)\\n        return x\\n\\n\\n####\\n\\ndf, img_dir = load_data()\\n    \\n###\\n\\ncheckpoint1 = torch.load('../input/shopee/v45.pth')\\ncheckpoint2 = torch.load('../input/shopee/v34.pth')\\ncheckpoint3 = torch.load('../input/shopee/v79.pth')\\nparams1 = checkpoint1['params']\\nparams2 = checkpoint2['params']\\nparams3 = checkpoint3['params']\\n\\ntransform = Compose([\\n    Resize(size=params1['test_size'] + 32, interpolation=Image.BICUBIC),\\n    CenterCrop((params1['test_size'], params1['test_size'])),\\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n])\\ndataset = ShopeeDataset(df=df, img_dir=img_dir, transform=None)\\ndata_loader = DataLoader(dataset, batch_size=8, shuffle=False,\\n                         drop_last=False, pin_memory=True, num_workers=NUM_WORKERS, collate_fn=lambda x: x)\\n\\nbackbone = timm.create_model(model_name=params1['backbone'], pretrained=False)\\nmodel1 = ShopeeNet(backbone, num_classes=0, fc_dim=params1['fc_dim'])\\nmodel1 = model1.to('cuda')\\nmodel1.load_state_dict(checkpoint1['model'], strict=False)\\nmodel1.train(False)\\nmodel1.p = params1['p_eval']\\n\\nbackbone = timm.create_model(model_name=params2['backbone'], pretrained=False)\\nmodel2 = ShopeeNet(backbone, num_classes=0, fc_dim=params2['fc_dim'])\\nmodel2 = model2.to('cuda')\\nmodel2.load_state_dict(checkpoint2['model'], strict=False)\\nmodel2.train(False)\\nmodel2.p = params2['p_eval']\\n\\nbackbone = timm.create_model(model_name=params3['backbone'], pretrained=False)\\ntokenizer = BertTokenizerFast(vocab_file='../input/bert-indo/vocab.txt')\\nbert_config = BertConfig.from_json_file('../input/bert-indo/config.json')\\nbert_model = BertModel(bert_config)\\nmodel3 = MultiModalNet(backbone, bert_model, num_classes=0, tokenizer=tokenizer, max_len=params3['max_len'],\\n                       fc_dim=params3['fc_dim'], s=params3['s'], margin=params3['margin'], loss=params3['loss'])\\nmodel3 = model3.to('cuda')\\nmodel3.load_state_dict(checkpoint3['model'], strict=False)\\nmodel3.train(False)\\nmodel3.p = params3['p_eval']\\n\\nimg_feats1 = []\\nimg_feats2 = []\\nmm_feats = []\\nimg_hs = []\\nimg_ws = []\\nst_sizes = []\\nfor batch in tqdm(data_loader, total=len(data_loader), miniters=None, ncols=55):\\n    img, title, h, w, st_size = list(zip(*batch))\\n    img = torch.cat([transform(x.to('cuda').float() / 255)[None] for x in img], axis=0)\\n    title = list(title)\\n    with torch.no_grad():\\n        feats_minibatch1 = model1.extract_feat(img)\\n        img_feats1.append(feats_minibatch1.cpu().numpy())\\n        feats_minibatch2 = model2.extract_feat(img)\\n        img_feats2.append(feats_minibatch2.cpu().numpy())\\n        feats_minibatch3 = model3.extract_feat(img, title)\\n        mm_feats.append(feats_minibatch3.cpu().numpy())\\n    img_hs.extend(list(h))\\n    img_ws.extend(list(w))\\n    st_sizes.extend(list(st_size))\\n\\nimg_feats1 = np.concatenate(img_feats1)\\nimg_feats1 /= np.linalg.norm(img_feats1, 2, axis=1, keepdims=True)\\nimg_feats2 = np.concatenate(img_feats2)\\nimg_feats2 /= np.linalg.norm(img_feats2, 2, axis=1, keepdims=True)\\nmm_feats = np.concatenate(mm_feats)\\nmm_feats /= np.linalg.norm(mm_feats, 2, axis=1, keepdims=True)\\n\\nnp.save('/tmp/img_feats1', img_feats1)\\nnp.save('/tmp/img_feats2', img_feats2)\\n\\nimg_feats = np.concatenate([\\n    img_feats1 * 1.0,\\n    img_feats2 * 1.0,\\n], axis=1)\\nimg_feats /= np.linalg.norm(img_feats, 2, axis=1, keepdims=True)\\n###\\n\\nnp.save('/tmp/img_feats', img_feats)\\n\\nres = faiss.StandardGpuResources()\\nindex_img = faiss.IndexFlatIP(params1['fc_dim'] + params2['fc_dim'])\\nindex_img = faiss.index_cpu_to_gpu(res, 0, index_img)\\nindex_img.add(img_feats)\\nsimilarities_img, indexes_img = index_img.search(img_feats, k)\\n\\n\\njoblib.dump([similarities_img, indexes_img], '/tmp/lyk_img_data.pkl')\\njoblib.dump([st_sizes, img_hs, img_ws], '/tmp/lyk_img_meta_data.pkl')\\n\\nres = faiss.StandardGpuResources()\\nindex_mm = faiss.IndexFlatIP(params3['fc_dim'])\\nindex_mm = faiss.index_cpu_to_gpu(res, 0, index_mm)\\nindex_mm.add(mm_feats)\\nsimilarities_mm, indexes_mm = index_mm.search(mm_feats, k)\\n\\njoblib.dump([similarities_mm, indexes_mm], '/tmp/lyk_mm_data.pkl')\\n\\n### for TKM\\nnp.save('/tmp/mm_feats', mm_feats)\\n\"' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from lyk_config import k, conf_th, DEBUG, load_data\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/timm045/')\n",
    "import timm\n",
    "\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize, RandomHorizontalFlip, ColorJitter, Normalize, Compose, RandomResizedCrop, CenterCrop, ToTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import joblib\n",
    "from scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\n",
    "import editdistance\n",
    "import networkx as nx\n",
    "from transformers import BertConfig, BertModel, BertTokenizerFast\n",
    "\n",
    "NUM_CLASSES = 11014\n",
    "NUM_WORKERS = 2\n",
    "SEED = 0\n",
    "\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "    \n",
    "class ShopeeNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 num_classes,\n",
    "                 fc_dim=512,\n",
    "                 s=30, margin=0.5, p=3):\n",
    "        super(ShopeeNet, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.backbone.reset_classifier(num_classes=0)  # remove classifier\n",
    "\n",
    "        self.fc = nn.Linear(self.backbone.num_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        self.p = p\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone.forward_features(x)\n",
    "        if isinstance(x, tuple):\n",
    "            x = (x[0] + x[1]) / 2\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            x = gem(x, p=self.p).view(batch_size, -1)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        feat = self.extract_feat(x)\n",
    "        x = self.loss_module(feat, label)\n",
    "        return x, feat\n",
    "\n",
    "\n",
    "class ShopeeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img = read_image(str(self.img_dir / row['image']))\n",
    "        _, h, w = img.shape\n",
    "        st_size = (self.img_dir / row['image']).stat().st_size\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, row['title'], h, w, st_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 bert_model,\n",
    "                 num_classes,\n",
    "                 tokenizer,\n",
    "                 max_len=32,\n",
    "                 fc_dim=512,\n",
    "                 s=30, margin=0.5, p=3, loss='ArcMarginProduct'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.backbone.reset_classifier(num_classes=0)  # remove classifier\n",
    "\n",
    "        self.bert_model = bert_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.fc = nn.Linear(self.bert_model.config.hidden_size + self.backbone.num_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        self.p = p\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def extract_feat(self, img, title):\n",
    "        batch_size = img.shape[0]\n",
    "        img = self.backbone.forward_features(img)\n",
    "        img = gem(img, p=self.p).view(batch_size, -1)\n",
    "\n",
    "        tokenizer_output = self.tokenizer(title, truncation=True, padding=True, max_length=self.max_len)\n",
    "        input_ids = torch.LongTensor(tokenizer_output['input_ids']).to('cuda')\n",
    "        token_type_ids = torch.LongTensor(tokenizer_output['token_type_ids']).to('cuda')\n",
    "        attention_mask = torch.LongTensor(tokenizer_output['attention_mask']).to('cuda')\n",
    "        title = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        # x = x.last_hidden_state.sum(dim=1) / attention_mask.sum(dim=1, keepdims=True)\n",
    "        title = title.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        x = torch.cat([img, title], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "df, img_dir = load_data()\n",
    "    \n",
    "###\n",
    "\n",
    "checkpoint1 = torch.load('../input/shopee/v45.pth')\n",
    "checkpoint2 = torch.load('../input/shopee/v34.pth')\n",
    "checkpoint3 = torch.load('../input/shopee/v79.pth')\n",
    "params1 = checkpoint1['params']\n",
    "params2 = checkpoint2['params']\n",
    "params3 = checkpoint3['params']\n",
    "\n",
    "transform = Compose([\n",
    "    Resize(size=params1['test_size'] + 32, interpolation=Image.BICUBIC),\n",
    "    CenterCrop((params1['test_size'], params1['test_size'])),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = ShopeeDataset(df=df, img_dir=img_dir, transform=None)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=False,\n",
    "                         drop_last=False, pin_memory=True, num_workers=NUM_WORKERS, collate_fn=lambda x: x)\n",
    "\n",
    "backbone = timm.create_model(model_name=params1['backbone'], pretrained=False)\n",
    "model1 = ShopeeNet(backbone, num_classes=0, fc_dim=params1['fc_dim'])\n",
    "model1 = model1.to('cuda')\n",
    "model1.load_state_dict(checkpoint1['model'], strict=False)\n",
    "model1.train(False)\n",
    "model1.p = params1['p_eval']\n",
    "\n",
    "backbone = timm.create_model(model_name=params2['backbone'], pretrained=False)\n",
    "model2 = ShopeeNet(backbone, num_classes=0, fc_dim=params2['fc_dim'])\n",
    "model2 = model2.to('cuda')\n",
    "model2.load_state_dict(checkpoint2['model'], strict=False)\n",
    "model2.train(False)\n",
    "model2.p = params2['p_eval']\n",
    "\n",
    "backbone = timm.create_model(model_name=params3['backbone'], pretrained=False)\n",
    "tokenizer = BertTokenizerFast(vocab_file='../input/bert-indo/vocab.txt')\n",
    "bert_config = BertConfig.from_json_file('../input/bert-indo/config.json')\n",
    "bert_model = BertModel(bert_config)\n",
    "model3 = MultiModalNet(backbone, bert_model, num_classes=0, tokenizer=tokenizer, max_len=params3['max_len'],\n",
    "                       fc_dim=params3['fc_dim'], s=params3['s'], margin=params3['margin'], loss=params3['loss'])\n",
    "model3 = model3.to('cuda')\n",
    "model3.load_state_dict(checkpoint3['model'], strict=False)\n",
    "model3.train(False)\n",
    "model3.p = params3['p_eval']\n",
    "\n",
    "img_feats1 = []\n",
    "img_feats2 = []\n",
    "mm_feats = []\n",
    "img_hs = []\n",
    "img_ws = []\n",
    "st_sizes = []\n",
    "for batch in tqdm(data_loader, total=len(data_loader), miniters=None, ncols=55):\n",
    "    img, title, h, w, st_size = list(zip(*batch))\n",
    "    img = torch.cat([transform(x.to('cuda').float() / 255)[None] for x in img], axis=0)\n",
    "    title = list(title)\n",
    "    with torch.no_grad():\n",
    "        feats_minibatch1 = model1.extract_feat(img)\n",
    "        img_feats1.append(feats_minibatch1.cpu().numpy())\n",
    "        feats_minibatch2 = model2.extract_feat(img)\n",
    "        img_feats2.append(feats_minibatch2.cpu().numpy())\n",
    "        feats_minibatch3 = model3.extract_feat(img, title)\n",
    "        mm_feats.append(feats_minibatch3.cpu().numpy())\n",
    "    img_hs.extend(list(h))\n",
    "    img_ws.extend(list(w))\n",
    "    st_sizes.extend(list(st_size))\n",
    "\n",
    "img_feats1 = np.concatenate(img_feats1)\n",
    "img_feats1 /= np.linalg.norm(img_feats1, 2, axis=1, keepdims=True)\n",
    "img_feats2 = np.concatenate(img_feats2)\n",
    "img_feats2 /= np.linalg.norm(img_feats2, 2, axis=1, keepdims=True)\n",
    "mm_feats = np.concatenate(mm_feats)\n",
    "mm_feats /= np.linalg.norm(mm_feats, 2, axis=1, keepdims=True)\n",
    "\n",
    "np.save('/tmp/img_feats1', img_feats1)\n",
    "np.save('/tmp/img_feats2', img_feats2)\n",
    "\n",
    "img_feats = np.concatenate([\n",
    "    img_feats1 * 1.0,\n",
    "    img_feats2 * 1.0,\n",
    "], axis=1)\n",
    "img_feats /= np.linalg.norm(img_feats, 2, axis=1, keepdims=True)\n",
    "###\n",
    "\n",
    "np.save('/tmp/img_feats', img_feats)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index_img = faiss.IndexFlatIP(params1['fc_dim'] + params2['fc_dim'])\n",
    "index_img = faiss.index_cpu_to_gpu(res, 0, index_img)\n",
    "index_img.add(img_feats)\n",
    "similarities_img, indexes_img = index_img.search(img_feats, k)\n",
    "\n",
    "\n",
    "joblib.dump([similarities_img, indexes_img], '/tmp/lyk_img_data.pkl')\n",
    "joblib.dump([st_sizes, img_hs, img_ws], '/tmp/lyk_img_meta_data.pkl')\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index_mm = faiss.IndexFlatIP(params3['fc_dim'])\n",
    "index_mm = faiss.index_cpu_to_gpu(res, 0, index_mm)\n",
    "index_mm.add(mm_feats)\n",
    "similarities_mm, indexes_mm = index_mm.search(mm_feats, k)\n",
    "\n",
    "joblib.dump([similarities_mm, indexes_mm], '/tmp/lyk_mm_data.pkl')\n",
    "\n",
    "### for TKM\n",
    "np.save('/tmp/mm_feats', mm_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015777,
     "end_time": "2021-05-11T17:20:53.457670",
     "exception": false,
     "start_time": "2021-05-11T17:20:53.441893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### image QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:20:53.497843Z",
     "iopub.status.busy": "2021-05-11T17:20:53.493969Z",
     "iopub.status.idle": "2021-05-11T17:20:55.265715Z",
     "shell.execute_reply": "2021-05-11T17:20:55.264576Z"
    },
    "papermill": {
     "duration": 1.792228,
     "end_time": "2021-05-11T17:20:55.265867",
     "exception": false,
     "start_time": "2021-05-11T17:20:53.473639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import gc\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def query_expansion(feats, sims, topk_idx, alpha=0.5, k=2):\n",
    "    weights = np.expand_dims(sims[:, :k] ** alpha, axis=-1).astype(np.float32)\n",
    "    feats = (feats[topk_idx[:, :k]] * weights).sum(axis=1)\n",
    "    return feats\n",
    "\n",
    "img_feats = np.load('/tmp/img_feats.npy')\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index_img = faiss.IndexFlatIP(img_feats.shape[1])\n",
    "index_img = faiss.index_cpu_to_gpu(res, 0, index_img)\n",
    "index_img.add(img_feats)\n",
    "img_D, img_I = index_img.search(img_feats, 60)\n",
    "\n",
    "np.save('/tmp/img_D', img_D)\n",
    "np.save('/tmp/img_I', img_I)\n",
    "\n",
    "img_feats_qe = query_expansion(img_feats, img_D, img_I)\n",
    "img_feats_qe /= np.linalg.norm(img_feats_qe, 2, axis=1, keepdims=True)\n",
    "\n",
    "img_feats = np.hstack([img_feats, img_feats_qe])\n",
    "img_feats /= np.linalg.norm(img_feats, axis=1).reshape((-1, 1))\n",
    "\n",
    "index = faiss.IndexFlatIP(img_feats.shape[1])\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "index.add(img_feats)\n",
    "img_D, img_I = index.search(img_feats, 60)\n",
    "\n",
    "np.save('/tmp/img_D_qe', img_D)\n",
    "np.save('/tmp/img_I_qe', img_I)\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016392,
     "end_time": "2021-05-11T17:20:55.299107",
     "exception": false,
     "start_time": "2021-05-11T17:20:55.282715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Multi-modal QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:20:55.337117Z",
     "iopub.status.busy": "2021-05-11T17:20:55.336387Z",
     "iopub.status.idle": "2021-05-11T17:20:56.975961Z",
     "shell.execute_reply": "2021-05-11T17:20:56.974997Z"
    },
    "papermill": {
     "duration": 1.660521,
     "end_time": "2021-05-11T17:20:56.976104",
     "exception": false,
     "start_time": "2021-05-11T17:20:55.315583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "import gc\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def query_expansion(feats, sims, topk_idx, alpha=0.5, k=2):\n",
    "    weights = np.expand_dims(sims[:, :k] ** alpha, axis=-1).astype(np.float32)\n",
    "    feats = (feats[topk_idx[:, :k]] * weights).sum(axis=1)\n",
    "    return feats\n",
    "\n",
    "mm_feats = np.load('/tmp/mm_feats.npy')\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index_mm = faiss.IndexFlatIP(mm_feats.shape[1])\n",
    "index_mm = faiss.index_cpu_to_gpu(res, 0, index_mm)\n",
    "index_mm.add(mm_feats)\n",
    "mm_D, mm_I = index_mm.search(mm_feats, 60)\n",
    "\n",
    "np.save('/tmp/mut_D', mm_D)\n",
    "np.save('/tmp/mut_I', mm_I)\n",
    "\n",
    "mm_feats_qe = query_expansion(mm_feats, mm_D, mm_I)\n",
    "mm_feats_qe /= np.linalg.norm(mm_feats_qe, 2, axis=1, keepdims=True)\n",
    "\n",
    "mm_feats = np.hstack([mm_feats, mm_feats_qe])\n",
    "mm_feats /= np.linalg.norm(mm_feats, axis=1).reshape((-1, 1))\n",
    "\n",
    "index = faiss.IndexFlatIP(mm_feats.shape[1])\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "index.add(mm_feats)\n",
    "mm_D, mm_I = index.search(mm_feats, 60)\n",
    "\n",
    "np.save('/tmp/mut_D_qe', mm_D)\n",
    "np.save('/tmp/mut_I_qe', mm_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016986,
     "end_time": "2021-05-11T17:20:57.012463",
     "exception": false,
     "start_time": "2021-05-11T17:20:56.995477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BERT similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:20:57.056016Z",
     "iopub.status.busy": "2021-05-11T17:20:57.052276Z",
     "iopub.status.idle": "2021-05-11T17:21:44.625823Z",
     "shell.execute_reply": "2021-05-11T17:21:44.626224Z"
    },
    "papermill": {
     "duration": 47.597142,
     "end_time": "2021-05-11T17:21:44.626382",
     "exception": false,
     "start_time": "2021-05-11T17:20:57.029240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 4/4 [00:05<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from lyk_config import k, conf_th, DEBUG, load_data\n",
    "import sys\n",
    "sys.path.append('../input/timm045/')\n",
    "import timm\n",
    "\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize, RandomHorizontalFlip, ColorJitter, Normalize, Compose, RandomResizedCrop, CenterCrop, ToTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import joblib\n",
    "from scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\n",
    "import editdistance\n",
    "import networkx as nx\n",
    "\n",
    "from transformers import BertConfig, BertModel, BertTokenizerFast\n",
    "\n",
    "NUM_CLASSES = 11014\n",
    "NUM_WORKERS = 2\n",
    "SEED = 0\n",
    "\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "\n",
    "class BertNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert_model,\n",
    "                 num_classes,\n",
    "                 tokenizer,\n",
    "                 max_len=32,\n",
    "                 fc_dim=512,\n",
    "                 simple_mean=True,\n",
    "                 s=30, margin=0.5, p=3, loss='ArcMarginProduct'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_model = bert_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.fc = nn.Linear(self.bert_model.config.hidden_size, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        self.p = p\n",
    "        self.simple_mean = simple_mean\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        tokenizer_output = self.tokenizer(x, truncation=True, padding=True, max_length=self.max_len)\n",
    "        if 'token_type_ids' in tokenizer_output:\n",
    "            input_ids = torch.LongTensor(tokenizer_output['input_ids']).to('cuda')\n",
    "            token_type_ids = torch.LongTensor(tokenizer_output['token_type_ids']).to('cuda')\n",
    "            attention_mask = torch.LongTensor(tokenizer_output['attention_mask']).to('cuda')\n",
    "            x = self.bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            input_ids = torch.LongTensor(tokenizer_output['input_ids']).to('cuda')\n",
    "            attention_mask = torch.LongTensor(tokenizer_output['attention_mask']).to('cuda')\n",
    "            x = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.simple_mean:\n",
    "            x = x.last_hidden_state.mean(dim=1)\n",
    "        else:\n",
    "            x = torch.sum(x.last_hidden_state * attention_mask.unsqueeze(-1), dim=1) / attention_mask.sum(dim=1, keepdims=True)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        if 'y' in row.keys():\n",
    "            target = torch.tensor(row['y'], dtype=torch.long)\n",
    "            return row['title'], target\n",
    "        else:\n",
    "            return row['title']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "df, img_dir = load_data()\n",
    "\n",
    "checkpoint = torch.load('../input/shopee/v75.pth')\n",
    "checkpoint2 = torch.load('../input/shopee/v102.pth')\n",
    "checkpoint3 = torch.load('../input/shopee/v103.pth')\n",
    "\n",
    "params_bert = checkpoint['params']\n",
    "params_bert2 = checkpoint2['params']\n",
    "params_bert3 = checkpoint3['params']\n",
    "\n",
    "datasets = {\n",
    "    'valid': BertDataset(df=df)\n",
    "}\n",
    "data_loaders = {\n",
    "    'valid': DataLoader(datasets['valid'], batch_size=params_bert['batch_size'] * 2, shuffle=False,\n",
    "                        drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "}\n",
    "\n",
    "tokenizer = BertTokenizerFast(vocab_file='../input/bert-indo/vocab.txt')\n",
    "bert_config = BertConfig.from_json_file('../input/bert-indo/config.json')\n",
    "bert_model = BertModel(bert_config)\n",
    "model = BertNet(bert_model, num_classes=0, tokenizer=tokenizer, max_len=params_bert['max_len'], simple_mean=True,\n",
    "                fc_dim=params_bert['fc_dim'], s=params_bert['s'], margin=params_bert['margin'], loss=params_bert['loss'])\n",
    "\n",
    "model = model.to('cuda')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)\n",
    "model.train(False)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "model_name = params_bert2['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained('../input/bertmultilingual/')\n",
    "bert_config = AutoConfig.from_pretrained('../input/bertmultilingual/')\n",
    "bert_model = AutoModel.from_config(bert_config)\n",
    "model2 = BertNet(bert_model, num_classes=0, tokenizer=tokenizer, max_len=params_bert['max_len'], simple_mean=False,\n",
    "                 fc_dim=params_bert['fc_dim'], s=params_bert['s'], margin=params_bert['margin'], loss=params_bert['loss'])\n",
    "model2 = model2.to('cuda')\n",
    "model2.load_state_dict(checkpoint2['model'], strict=False)\n",
    "model2.train(False)\n",
    "\n",
    "#########\n",
    "\n",
    "model_name = params_bert3['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained('../input/bertxlm/')\n",
    "bert_config = AutoConfig.from_pretrained('../input/bertxlm/')\n",
    "bert_model = AutoModel.from_config(bert_config)\n",
    "model3 = BertNet(bert_model, num_classes=0, tokenizer=tokenizer, max_len=params_bert3['max_len'], simple_mean=False,\n",
    "                 fc_dim=params_bert3['fc_dim'], s=params_bert3['s'], margin=params_bert3['margin'], loss=params_bert3['loss'])\n",
    "model3 = model3.to('cuda')\n",
    "model3.load_state_dict(checkpoint3['model'], strict=False)\n",
    "model3.train(False)\n",
    "\n",
    "bert_feats1 = []\n",
    "bert_feats2 = []\n",
    "bert_feats3 = []\n",
    "for i, title in tqdm(enumerate(data_loaders['valid']),\n",
    "                     total=len(data_loaders['valid']), miniters=None, ncols=55):\n",
    "    with torch.no_grad():\n",
    "        bert_feats_minibatch = model.extract_feat(title)\n",
    "        bert_feats1.append(bert_feats_minibatch.cpu().numpy())\n",
    "        bert_feats_minibatch = model2.extract_feat(title)\n",
    "        bert_feats2.append(bert_feats_minibatch.cpu().numpy())\n",
    "        bert_feats_minibatch = model3.extract_feat(title)\n",
    "        bert_feats3.append(bert_feats_minibatch.cpu().numpy())\n",
    "\n",
    "bert_feats1 = np.concatenate(bert_feats1)\n",
    "bert_feats1 /= np.linalg.norm(bert_feats1, 2, axis=1, keepdims=True)\n",
    "bert_feats2 = np.concatenate(bert_feats2)\n",
    "bert_feats2 /= np.linalg.norm(bert_feats2, 2, axis=1, keepdims=True)\n",
    "bert_feats3 = np.concatenate(bert_feats3)\n",
    "bert_feats3 /= np.linalg.norm(bert_feats3, 2, axis=1, keepdims=True)\n",
    "\n",
    "bert_feats = np.concatenate([bert_feats1, bert_feats2], axis=1)\n",
    "bert_feats /= np.linalg.norm(bert_feats, 2, axis=1, keepdims=True)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index_bert = faiss.IndexFlatIP(params_bert['fc_dim'])\n",
    "index_bert = faiss.index_cpu_to_gpu(res, 0, index_bert)\n",
    "index_bert.add(bert_feats1)\n",
    "similarities_bert, indexes_bert = index_bert.search(bert_feats1, k)\n",
    "\n",
    "np.save('/tmp/bert_feats1', bert_feats1)\n",
    "np.save('/tmp/bert_feats2', bert_feats2)\n",
    "np.save('/tmp/bert_feats3', bert_feats3)\n",
    "\n",
    "bert_feats = np.concatenate([bert_feats1, bert_feats2, bert_feats3], axis=1)\n",
    "bert_feats /= np.linalg.norm(bert_feats, 2, axis=1, keepdims=True)\n",
    "\n",
    "np.save('/tmp/bert_feats', bert_feats)\n",
    "\n",
    "joblib.dump([similarities_bert, indexes_bert], '/tmp/lyk_bert_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016927,
     "end_time": "2021-05-11T17:21:44.660401",
     "exception": false,
     "start_time": "2021-05-11T17:21:44.643474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Bert QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:21:44.702910Z",
     "iopub.status.busy": "2021-05-11T17:21:44.698995Z",
     "iopub.status.idle": "2021-05-11T17:21:46.813087Z",
     "shell.execute_reply": "2021-05-11T17:21:46.813671Z"
    },
    "papermill": {
     "duration": 2.136413,
     "end_time": "2021-05-11T17:21:46.813858",
     "exception": false,
     "start_time": "2021-05-11T17:21:44.677445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import gc\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def query_expansion(feats, sims, topk_idx, alpha=0.5, k=2):\n",
    "    weights = np.expand_dims(sims[:, :k] ** alpha, axis=-1).astype(np.float32)\n",
    "    feats = (feats[topk_idx[:, :k]] * weights).sum(axis=1)\n",
    "    return feats\n",
    "\n",
    "brt_feats = np.load('/tmp/bert_feats.npy')\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index_brt = faiss.IndexFlatIP(brt_feats.shape[1])\n",
    "index_brt = faiss.index_cpu_to_gpu(res, 0, index_brt)\n",
    "index_brt.add(brt_feats)\n",
    "brt_D, brt_I = index_brt.search(brt_feats, 60)\n",
    "\n",
    "np.save('/tmp/brt_D', brt_D)\n",
    "np.save('/tmp/brt_I', brt_I)\n",
    "\n",
    "del index_brt\n",
    "gc.collect()\n",
    "\n",
    "brt_feats_qe = query_expansion(brt_feats, brt_D, brt_I)\n",
    "brt_feats_qe /= np.linalg.norm(brt_feats_qe, 2, axis=1, keepdims=True)\n",
    "\n",
    "brt_feats = np.hstack([brt_feats, brt_feats_qe])\n",
    "brt_feats /= np.linalg.norm(brt_feats, axis=1).reshape((-1, 1))\n",
    "\n",
    "index = faiss.IndexFlatIP(brt_feats.shape[1])\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "index.add(brt_feats)\n",
    "brt_D, brt_I = index.search(brt_feats, 60)\n",
    "\n",
    "np.save('/tmp/brt_D_qe', brt_D)\n",
    "np.save('/tmp/brt_I_qe', brt_I)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017532,
     "end_time": "2021-05-11T17:21:46.849665",
     "exception": false,
     "start_time": "2021-05-11T17:21:46.832133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image & BERT similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:21:46.892128Z",
     "iopub.status.busy": "2021-05-11T17:21:46.889134Z",
     "iopub.status.idle": "2021-05-11T17:21:48.795812Z",
     "shell.execute_reply": "2021-05-11T17:21:48.797534Z"
    },
    "papermill": {
     "duration": 1.930282,
     "end_time": "2021-05-11T17:21:48.797775",
     "exception": false,
     "start_time": "2021-05-11T17:21:46.867493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2304)\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import gc\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def query_expansion(feats, sims, topk_idx, alpha=0.5, k=2):\n",
    "    weights = np.expand_dims(sims[:, :k] ** alpha, axis=-1).astype(np.float32)\n",
    "    feats = (feats[topk_idx[:, :k]] * weights).sum(axis=1)\n",
    "    return feats\n",
    "\n",
    "\n",
    "feats_bert = np.load('/tmp/bert_feats.npy')\n",
    "feats_img = np.load('/tmp/img_feats.npy')\n",
    "\n",
    "bth_feats = np.hstack([feats_bert, feats_img])\n",
    "bth_feats /= np.linalg.norm(bth_feats, 2, axis=1, keepdims=True)\n",
    "\n",
    "print(bth_feats.shape)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.IndexFlatIP(bth_feats.shape[1])\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "index.add(bth_feats)\n",
    "\n",
    "bth_D, bth_I = index.search(bth_feats, 60)\n",
    "np.save('/tmp/bth_D', bth_D)\n",
    "np.save('/tmp/bth_I', bth_I)\n",
    "\n",
    "del index\n",
    "gc.collect()\n",
    "\n",
    "bth_feats_qe = query_expansion(bth_feats, bth_D, bth_I)\n",
    "bth_feats_qe /= np.linalg.norm(bth_feats_qe, 2, axis=1, keepdims=True)\n",
    "\n",
    "bth_feats = np.hstack([bth_feats, bth_feats_qe])\n",
    "bth_feats /= np.linalg.norm(bth_feats, axis=1).reshape((-1, 1))\n",
    "\n",
    "index = faiss.IndexFlatIP(bth_feats.shape[1])\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "index.add(bth_feats)\n",
    "bth_D, bth_I = index.search(bth_feats, 60)\n",
    "\n",
    "np.save('/tmp/bth_D_qe', bth_D)\n",
    "np.save('/tmp/bth_I_qe', bth_I)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03151,
     "end_time": "2021-05-11T17:21:48.861563",
     "exception": false,
     "start_time": "2021-05-11T17:21:48.830053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# lyakaap Side (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:21:48.936915Z",
     "iopub.status.busy": "2021-05-11T17:21:48.933906Z",
     "iopub.status.idle": "2021-05-11T17:22:05.474607Z",
     "shell.execute_reply": "2021-05-11T17:22:05.473844Z"
    },
    "papermill": {
     "duration": 16.581301,
     "end_time": "2021-05-11T17:22:05.474767",
     "exception": false,
     "start_time": "2021-05-11T17:21:48.893466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  load - done in 0.12970s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 266.72it/s]\n",
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from lyk_config import k, conf_th, DEBUG, load_data\n",
    "import sys\n",
    "sys.path.append('../input/timm045/')\n",
    "import timm\n",
    "\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\n",
    "import editdistance\n",
    "import networkx as nx\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "NUM_CLASSES = 11014\n",
    "NUM_WORKERS = 2\n",
    "SEED = 0\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "\n",
    "    def __init__(self, feats=None, labels=None, weights=None, pair_tuples=None, k=50, top_neighbors=None):\n",
    "        self.feats = feats\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.pair_tuples = pair_tuples\n",
    "        self.k = k\n",
    "        self.top_neighbors = top_neighbors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i, j = self.pair_tuples[index]\n",
    "        feat = torch.FloatTensor(self.feats[i][j])\n",
    "\n",
    "        padding_i = [[0] * feat.shape[0]] * (self.k - len(self.top_neighbors[i]))\n",
    "        neighbor_feats_i = torch.FloatTensor([\n",
    "            self.feats[i][neighbor]\n",
    "            for neighbor in self.top_neighbors[i]\n",
    "        ] + padding_i)\n",
    "        padding_j = [[0] * feat.shape[0]] * (self.k - len(self.top_neighbors[j]))\n",
    "        neighbor_feats_j = torch.FloatTensor([\n",
    "            self.feats[j][neighbor]\n",
    "            for neighbor in self.top_neighbors[j]\n",
    "        ] + padding_j)\n",
    "        neighbor_feats = torch.cat([feat.unsqueeze(0), neighbor_feats_i, neighbor_feats_j], dim=0)\n",
    "\n",
    "        outputs = (feat, neighbor_feats)\n",
    "        if self.labels is not None:\n",
    "            outputs += (self.labels[i] == self.labels[j],)\n",
    "        if self.weights is not None:\n",
    "            outputs += (self.weights[i],)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_tuples)\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h):\n",
    "        Wh = h @ self.W  # h.shape: (B, N, in_features), Wh.shape: (B, N, out_features)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(3))\n",
    "\n",
    "        attention = F.softmax(e, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        B, N, D = Wh.shape\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=1)\n",
    "        Wh_repeated_alternating = Wh.repeat(1, N, 1)\n",
    "\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=2)\n",
    "        return all_combinations_matrix.view(-1, N, N, 2 * D)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GATPairClassifier(nn.Module):\n",
    "    def __init__(self, nfeat, nhid=8, nclass=1, dropout=0.6, alpha=0.2, nheads=8, pooling='first'):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nhid, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(nfeat + nhid, nhid),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(nhid),\n",
    "            nn.Linear(nhid, nclass),\n",
    "        )\n",
    "\n",
    "    def forward_gat(self, x):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x) for att in self.attentions], dim=2)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x))\n",
    "        if self.pooling == 'first':\n",
    "            return x[:, 0]\n",
    "        elif self.pooling == 'mean':\n",
    "            return x.mean(dim=1)\n",
    "\n",
    "    def forward(self, feats, neighbor_feats):\n",
    "        gat_feats = self.forward_gat(neighbor_feats)\n",
    "        cat_feats = torch.cat([feats, gat_feats], dim=1)\n",
    "        return self.classifier(cat_feats).squeeze(1)\n",
    "\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "map_used_time = defaultdict(float)\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    tt = time.time() - t0\n",
    "    map_used_time[title] += tt\n",
    "    print(\"  {} - done in {:.5f}s\".format(title, tt))\n",
    "\n",
    "\n",
    "df, img_dir = load_data()\n",
    "\n",
    "stop_words = set([\n",
    "    'promo','diskon','baik','terbaik', 'murah',\n",
    "    'termurah', 'harga', 'price', 'best', 'seller',\n",
    "    'bestseller', 'ready', 'stock', 'stok', 'limited',\n",
    "    'bagus', 'kualitas', 'berkualitas', 'hari', 'ini',\n",
    "    'jadi', 'gratis',\n",
    "])\n",
    "\n",
    "\n",
    "titles = [\n",
    "    title.translate(str.maketrans({_: ' ' for _ in string.punctuation}))\n",
    "    for title in df['title'].str.lower().values\n",
    "]\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, \n",
    "                                   binary=True, \n",
    "                                   min_df=2, \n",
    "                                   token_pattern='(?u)\\\\b\\\\w+\\\\b', \n",
    "                                   tokenizer=tokenizer.tokenize,\n",
    "                                   dtype=np.float32,\n",
    "                                   norm='l2')\n",
    "tfidf_feats = tfidf_vectorizer.fit_transform(titles)\n",
    "simmat_tfidf = tfidf_feats @ tfidf_feats.T\n",
    "\n",
    "with timer('load'):\n",
    "    st_sizes, img_hs, img_ws = joblib.load('/tmp/lyk_img_meta_data.pkl')\n",
    "    similarities_img = np.load('/tmp/img_D_qe.npy')[:, :k]\n",
    "    indexes_img = np.load('/tmp/img_I_qe.npy')[:, :k]\n",
    "\n",
    "    similarities_bert = np.load('/tmp/brt_D_qe.npy')[:, :k]\n",
    "    indexes_bert = np.load('/tmp/brt_I_qe.npy')[:, :k]\n",
    "\n",
    "    similarities_mm = np.load('/tmp/mut_D_qe.npy')[:, :k]\n",
    "    indexes_mm = np.load('/tmp/mut_I_qe.npy')[:, :k]\n",
    "    \n",
    "    row = indexes_bert.ravel()\n",
    "    col = np.arange(len(indexes_bert)).repeat(k)\n",
    "    data = similarities_bert.ravel()\n",
    "    simmat_bert = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "\n",
    "    row = indexes_img.ravel()\n",
    "    col = np.arange(len(indexes_img)).repeat(k)\n",
    "    data = similarities_img.ravel()\n",
    "    simmat_img = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "\n",
    "    row = indexes_mm.ravel()\n",
    "    col = np.arange(len(indexes_mm)).repeat(k)\n",
    "    data = similarities_mm.ravel()\n",
    "    simmat_mm = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "\n",
    "del row, col, data\n",
    "gc.collect()\n",
    "\n",
    "ckpt = torch.load('../input/shopee-meta-models/v135.pth')\n",
    "params = ckpt['params']\n",
    "\n",
    "top_neighbors = defaultdict(list)\n",
    "feats = defaultdict(lambda: defaultdict())\n",
    "\n",
    "pair_tuples = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    right_indexes = set(indexes_img[i, :k].tolist() + indexes_bert[i, :k].tolist())\n",
    "    right_indexes.remove(i)  # remove self\n",
    "\n",
    "    right_indexes = list(right_indexes)\n",
    "    scores = {}\n",
    "    for j in right_indexes:\n",
    "        pair_tuples.append((i, j))\n",
    "\n",
    "        sim_img = simmat_img.get((i, j), 0)\n",
    "        sim_bert = simmat_bert.get((i, j), 0)\n",
    "        sim_mm = simmat_mm.get((i, j), 0)\n",
    "        sim_tfidf = simmat_tfidf[i, j]\n",
    "        if sim_img == 0 and sim_bert == 0:\n",
    "            continue\n",
    "\n",
    "        feats[i][j] = [\n",
    "            sim_img,\n",
    "            sim_tfidf,\n",
    "            sim_bert,\n",
    "            sim_mm,\n",
    "        ]\n",
    "        scores[j] = sim_img + sim_tfidf + sim_bert + sim_mm\n",
    "\n",
    "    top_neighbors[i] = sorted(right_indexes, key=lambda x: scores[x], reverse=True)[:params['k']]\n",
    "\n",
    "dataset = GraphDataset(\n",
    "    feats=feats,\n",
    "    pair_tuples=pair_tuples,\n",
    "    k=params['k'],\n",
    "    top_neighbors=top_neighbors,\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=2 ** 12, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "gat = GATPairClassifier(nfeat=len(feats[i][j]), nhid=params['nhid'],\n",
    "                        dropout=params['dropout'], nheads=params['nheads'], pooling=params['pooling'])\n",
    "gat.to('cuda').eval()\n",
    "gat.load_state_dict(ckpt['model'])\n",
    "\n",
    "del tfidf_feats\n",
    "gc.collect()\n",
    "###\n",
    "\n",
    "preds = []\n",
    "for feats, neighbor_feats in tqdm(loader, desc='predict', leave=False):\n",
    "    feats = feats.to('cuda', non_blocking=True)\n",
    "    neighbor_feats = neighbor_feats.to('cuda', non_blocking=True)\n",
    "    with torch.no_grad():\n",
    "        pred = gat(feats, neighbor_feats).sigmoid().detach().cpu().numpy().tolist()\n",
    "        preds.extend(pred)\n",
    "\n",
    "conf_th_gcn = 0.3\n",
    "df_pair = pd.DataFrame()\n",
    "col, row = list(zip(*pair_tuples))\n",
    "df_pair['i'] = col\n",
    "df_pair['j'] = row\n",
    "\n",
    "df_pair['posting_id'] = df['posting_id'].values[df_pair['i'].values]\n",
    "df_pair['posting_id_target'] = df['posting_id'].values[df_pair['j'].values]\n",
    "\n",
    "df_pair = df_pair[['posting_id', 'posting_id_target']]\n",
    "df_pair['pred'] = preds\n",
    "df_pair['pred'] -= conf_th_gcn\n",
    "\n",
    "df_pair.to_pickle('submission_lyak_gcn.pkl')\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018588,
     "end_time": "2021-05-11T17:22:05.512610",
     "exception": false,
     "start_time": "2021-05-11T17:22:05.494022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# lyakaap Side (LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:22:05.560517Z",
     "iopub.status.busy": "2021-05-11T17:22:05.556432Z",
     "iopub.status.idle": "2021-05-11T17:22:27.076520Z",
     "shell.execute_reply": "2021-05-11T17:22:27.077048Z"
    },
    "papermill": {
     "duration": 21.546145,
     "end_time": "2021-05-11T17:22:27.077208",
     "exception": false,
     "start_time": "2021-05-11T17:22:05.531063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  load - done in 0.10397s\n",
      "  to_frame - done in 0.03809s\n",
      "  sim_tfidf - done in 0.01660s\n",
      "[17:22:23] ../src/frontend/lightgbm.cc:544: model.num_tree = 1630\n",
      "[17:22:23] ../src/frontend/lightgbm.cc:544: model.num_tree = 1630\n",
      "[17:22:24] ../src/frontend/lightgbm.cc:544: model.num_tree = 1630\n",
      "[17:22:24] ../src/frontend/lightgbm.cc:544: model.num_tree = 1630\n",
      "[17:22:24] ../src/frontend/lightgbm.cc:544: model.num_tree = 1630\n",
      "  predict - done in 0.60464s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 246.28it/s]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from lyk_config import k, conf_th, DEBUG, load_data\n",
    "import sys\n",
    "sys.path.append('../input/timm045/')\n",
    "import timm\n",
    "\n",
    "from itertools import zip_longest\n",
    "import json\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack, vstack, csc_matrix, csr_matrix\n",
    "import editdistance\n",
    "import networkx as nx\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "NUM_CLASSES = 11014\n",
    "NUM_WORKERS = 2\n",
    "SEED = 0\n",
    "\n",
    "###\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "map_used_time = defaultdict(float)\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    tt = time.time() - t0\n",
    "    map_used_time[title] += tt\n",
    "    print(\"  {} - done in {:.5f}s\".format(title, tt))\n",
    "\n",
    "\n",
    "df, img_dir = load_data()\n",
    "\n",
    "stop_words = set([\n",
    "    'promo','diskon','baik','terbaik', 'murah',\n",
    "    'termurah', 'harga', 'price', 'best', 'seller',\n",
    "    'bestseller', 'ready', 'stock', 'stok', 'limited',\n",
    "    'bagus', 'kualitas', 'berkualitas', 'hari', 'ini',\n",
    "    'jadi', 'gratis',\n",
    "])\n",
    "\n",
    "titles = [\n",
    "    title.translate(str.maketrans({_: ' ' for _ in string.punctuation}))\n",
    "    for title in df['title'].str.lower().values\n",
    "]\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, \n",
    "                                   binary=True, \n",
    "                                   min_df=2, \n",
    "                                   token_pattern='(?u)\\\\b\\\\w+\\\\b', \n",
    "                                   tokenizer=tokenizer.tokenize,\n",
    "                                   dtype=np.float32,\n",
    "                                   norm='l2')\n",
    "tfidf_feats = tfidf_vectorizer.fit_transform(titles)\n",
    "\n",
    "with timer('load'):\n",
    "    similarities_bert, indexes_bert = joblib.load('/tmp/lyk_bert_data.pkl')\n",
    "    similarities_img, indexes_img = joblib.load('/tmp/lyk_img_data.pkl')\n",
    "    st_sizes, img_hs, img_ws = joblib.load('/tmp/lyk_img_meta_data.pkl')\n",
    "    similarities_mm, indexes_mm = joblib.load('/tmp/lyk_mm_data.pkl')\n",
    "    \n",
    "    row = indexes_bert.ravel()\n",
    "    col = np.arange(len(indexes_bert)).repeat(k)\n",
    "    data = similarities_bert.ravel()\n",
    "    simmat_bert = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "\n",
    "    row = indexes_img.ravel()\n",
    "    col = np.arange(len(indexes_img)).repeat(k)\n",
    "    data = similarities_img.ravel()\n",
    "    simmat_img = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "\n",
    "    row = indexes_mm.ravel()\n",
    "    col = np.arange(len(indexes_mm)).repeat(k)\n",
    "    data = similarities_mm.ravel()\n",
    "    simmat_mm = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "\n",
    "del row, col, data\n",
    "gc.collect()\n",
    "\n",
    "mean_sim_img_top5 = similarities_img[:, :5].mean(1)\n",
    "mean_sim_bert_top5 = similarities_bert[:, :5].mean(1)\n",
    "mean_mean_sim_img_top5 = mean_sim_img_top5[indexes_img[:, :5]].mean(1)\n",
    "mean_mean_sim_bert_top5 = mean_sim_bert_top5[indexes_bert[:, :5]].mean(1)\n",
    "\n",
    "mean_sim_img_top5 = (mean_sim_img_top5 - mean_sim_img_top5.mean()) / mean_sim_img_top5.std()\n",
    "mean_sim_bert_top5 = (mean_sim_bert_top5 - mean_sim_bert_top5.mean()) / mean_sim_bert_top5.std()\n",
    "mean_mean_sim_img_top5 = (mean_mean_sim_img_top5 - mean_mean_sim_img_top5.mean()) / mean_mean_sim_img_top5.std()\n",
    "mean_mean_sim_bert_top5 = (mean_mean_sim_bert_top5 - mean_mean_sim_bert_top5.mean()) / mean_mean_sim_bert_top5.std()\n",
    "\n",
    "mean_sim_img_top15 = similarities_img[:, :15].mean(1)\n",
    "mean_sim_bert_top15 = similarities_bert[:, :15].mean(1)\n",
    "mean_sim_img_top15 = (mean_sim_img_top15 - mean_sim_img_top15.mean()) / mean_sim_img_top15.std()\n",
    "mean_sim_bert_top15 = (mean_sim_bert_top15 - mean_sim_bert_top15.mean()) / mean_sim_bert_top15.std()\n",
    "\n",
    "mean_sim_img_top30 = similarities_img[:, :30].mean(1)\n",
    "mean_sim_bert_top30 = similarities_bert[:, :30].mean(1)\n",
    "mean_sim_img_top30 = (mean_sim_img_top30 - mean_sim_img_top30.mean()) / mean_sim_img_top30.std()\n",
    "mean_sim_bert_top30 = (mean_sim_bert_top30 - mean_sim_bert_top30.mean()) / mean_sim_bert_top30.std()\n",
    "\n",
    "mean_sim_mm_top5 = similarities_mm[:, :5].mean(1)\n",
    "mean_mean_sim_mm_top5 = mean_sim_mm_top5[indexes_mm[:, :5]].mean(1)\n",
    "\n",
    "mean_sim_mm_top5 = (mean_sim_mm_top5 - mean_sim_mm_top5.mean()) / mean_sim_mm_top5.std()\n",
    "mean_mean_sim_mm_top5 = (mean_mean_sim_mm_top5 - mean_mean_sim_mm_top5.mean()) / mean_mean_sim_mm_top5.std()\n",
    "\n",
    "mean_sim_mm_top15 = similarities_mm[:, :15].mean(1)\n",
    "mean_sim_mm_top15 = (mean_sim_mm_top15 - mean_sim_mm_top15.mean()) / mean_sim_mm_top15.std()\n",
    "\n",
    "mean_sim_mm_top30 = similarities_mm[:, :30].mean(1)\n",
    "mean_sim_mm_top30 = (mean_sim_mm_top30 - mean_sim_mm_top30.mean()) / mean_sim_mm_top30.std()\n",
    "\n",
    "row_titles = df['title'].values\n",
    "posting_ids = df['posting_id'].values\n",
    "\n",
    "tmp_dir = Path('/tmp/rows')\n",
    "tmp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "rows = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    right_indexes = set(indexes_img[i].tolist() + indexes_bert[i].tolist())\n",
    "\n",
    "    for _, j in enumerate(right_indexes):\n",
    "        if i == j:\n",
    "            continue\n",
    "        sim_img = simmat_img.get((i, j), 0)\n",
    "        sim_bert = simmat_bert.get((i, j), 0)\n",
    "        sim_mm = simmat_mm.get((i, j), 0)\n",
    "        if sim_img == 0 and sim_bert == 0:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            'i': i,\n",
    "            'j': j,\n",
    "            'posting_id': posting_ids[i],\n",
    "            'posting_id_target': posting_ids[j],\n",
    "            'sim_img': sim_img,\n",
    "            'sim_bert': sim_bert,\n",
    "            'sim_mm': sim_mm,\n",
    "            'edit_distance': editdistance.eval(titles[i], titles[j]),\n",
    "            'title_len': len(row_titles[i]),\n",
    "            'title_len_target': len(row_titles[j]),\n",
    "            'title_num_words': len(row_titles[i].split()),\n",
    "            'title_num_words_target': len(row_titles[j].split()),\n",
    "            'mean_sim_img_top5': mean_sim_img_top5[i],\n",
    "            'mean_sim_img_target_top5': mean_sim_img_top5[j],\n",
    "            'mean_sim_bert_top5': mean_sim_bert_top5[i],\n",
    "            'mean_sim_bert_target_top5': mean_sim_bert_top5[j],\n",
    "            'mean_sim_img_top15': mean_sim_img_top15[i],\n",
    "            'mean_sim_img_target_top15': mean_sim_img_top15[j],\n",
    "            'mean_sim_bert_top15': mean_sim_bert_top15[i],\n",
    "            'mean_sim_bert_target_top15': mean_sim_bert_top15[j],\n",
    "            'mean_sim_img_top30': mean_sim_img_top30[i],\n",
    "            'mean_sim_img_target_top30': mean_sim_img_top30[j],\n",
    "            'mean_sim_bert_top30': mean_sim_bert_top30[i],\n",
    "            'mean_sim_bert_target_top30': mean_sim_bert_top30[j],\n",
    "            'st_size': st_sizes[i],\n",
    "            'st_size_target': st_sizes[j],\n",
    "            'wxh/st_size': img_ws[i] * img_hs[i] / st_sizes[i],\n",
    "            'wxh/st_size_target': img_ws[j] * img_hs[j] / st_sizes[j],\n",
    "            'mean_mean_sim_img_top5': mean_mean_sim_img_top5[i],\n",
    "            'mean_mean_sim_img_target_top5': mean_mean_sim_img_top5[j],\n",
    "            'mean_mean_sim_bert_top5': mean_mean_sim_bert_top5[i],\n",
    "            'mean_mean_sim_bert_target_top5': mean_mean_sim_bert_top5[j],\n",
    "            'mean_sim_mm_top5': mean_sim_mm_top5[i],\n",
    "            'mean_sim_mm_target_top5': mean_sim_mm_top5[j],\n",
    "            'mean_sim_mm_top15': mean_sim_mm_top15[i],\n",
    "            'mean_sim_mm_target_top15': mean_sim_mm_top15[j],\n",
    "            'mean_sim_mm_top30': mean_sim_mm_top30[i],\n",
    "            'mean_sim_mm_target_top30': mean_sim_mm_top30[j],\n",
    "            'mean_mean_sim_mm_top5': mean_mean_sim_mm_top5[i],\n",
    "            'mean_mean_sim_mm_target_top5': mean_mean_sim_mm_top5[j],\n",
    "        })\n",
    "\n",
    "    if i % 10000 == 9999 or i == len(df) - 1:\n",
    "        tmp_df = pd.DataFrame(rows)\n",
    "        for col in tmp_df.columns:\n",
    "            if tmp_df[col].dtype == 'float64':\n",
    "                tmp_df[col] = tmp_df[col].astype('float32')\n",
    "            elif tmp_df[col].dtype == 'int64':\n",
    "                tmp_df[col] = tmp_df[col].astype('int32')\n",
    "        tmp_df.to_feather(tmp_dir / f'{i}.feather')\n",
    "        rows = []\n",
    "\n",
    "df.drop(['image', 'title'], axis=1, inplace=True)\n",
    "del (\n",
    "    mean_sim_img_top5, mean_sim_img_top15, mean_sim_img_top30, mean_mean_sim_img_top5,\n",
    "    mean_sim_bert_top5, mean_sim_bert_top15, mean_sim_bert_top30, mean_mean_sim_bert_top5,\n",
    "    mean_sim_mm_top5, mean_sim_mm_top15, mean_sim_mm_top30, mean_mean_sim_mm_top5,\n",
    "    simmat_img, simmat_bert, simmat_mm,\n",
    "    similarities_img, indexes_img,\n",
    "    similarities_bert, indexes_bert,\n",
    "    similarities_mm, indexes_mm,\n",
    ")\n",
    "gc.collect()\n",
    "with timer('to_frame'):\n",
    "    df_pair = pd.concat([pd.read_feather(path) for path in tmp_dir.glob('**/*.feather')], axis=0).reset_index(drop=True)\n",
    "del rows\n",
    "gc.collect()\n",
    "\n",
    "with timer('sim_tfidf'):\n",
    "    df_pair['sim_tfidf'] = tfidf_feats[df_pair['i'].values].multiply(tfidf_feats[df_pair['j'].values]).sum(axis=1)\n",
    "df_pair['title_len_diff'] = np.abs(df_pair['title_len'] - df_pair['title_len_target'])\n",
    "df_pair['title_num_words_diff'] = np.abs(df_pair['title_num_words'] - df_pair['title_num_words_target'])\n",
    "\n",
    "del tfidf_feats\n",
    "gc.collect()\n",
    "###\n",
    "\n",
    "from cuml import ForestInference\n",
    "import treelite\n",
    "list_clf = []\n",
    "for clf in joblib.load('../input/shopee/boosters_v34_v45_mm.pickle'):\n",
    "    clf.save_model('/tmp/tmp.lgb')\n",
    "    fi = ForestInference()\n",
    "    fi.load_from_treelite_model(treelite.Model.load('/tmp/tmp.lgb', model_format='lightgbm'))\n",
    "    list_clf.append(fi)\n",
    "\n",
    "X = df_pair[[\n",
    "    'sim_img', 'sim_tfidf', 'sim_bert', 'sim_mm', 'edit_distance',\n",
    "    'title_len', 'title_len_target', 'title_len_diff',\n",
    "    'title_num_words', 'title_num_words_target', 'title_num_words_diff',\n",
    "    'mean_sim_img_top5', 'mean_sim_img_target_top5',\n",
    "    'mean_sim_bert_top5', 'mean_sim_bert_target_top5',\n",
    "    'mean_sim_mm_top5', 'mean_sim_mm_target_top5',\n",
    "    'mean_sim_img_top15', 'mean_sim_img_target_top15',\n",
    "    'mean_sim_bert_top15', 'mean_sim_bert_target_top15',\n",
    "    'mean_sim_mm_top15', 'mean_sim_mm_target_top15',\n",
    "    'mean_sim_img_top30', 'mean_sim_img_target_top30',\n",
    "    'mean_sim_bert_top30', 'mean_sim_bert_target_top30',\n",
    "    'mean_sim_mm_top30', 'mean_sim_mm_target_top30',\n",
    "    'st_size', 'st_size_target',\n",
    "    'wxh/st_size', 'wxh/st_size_target',\n",
    "    'mean_mean_sim_img_top5', 'mean_mean_sim_img_target_top5',\n",
    "    'mean_mean_sim_bert_top5', 'mean_mean_sim_bert_target_top5',\n",
    "    'mean_mean_sim_mm_top5', 'mean_mean_sim_mm_target_top5',\n",
    "]]\n",
    "\n",
    "## passing as cupy array might be able to avoid multipy copy to GPU.\n",
    "X = cp.asarray(X[clf.feature_name()].values.astype(np.float32))\n",
    "df_pair = df_pair[['posting_id', 'posting_id_target']]\n",
    "\n",
    "gc.collect()\n",
    "with timer('predict'):\n",
    "    df_pair['pred'] = np.mean([clf.predict(X).get() for clf in list_clf], axis=0) - conf_th\n",
    "\n",
    "df_pair.to_pickle('submission_lyak.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019292,
     "end_time": "2021-05-11T17:22:27.116611",
     "exception": false,
     "start_time": "2021-05-11T17:22:27.097319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TKM side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:22:27.163846Z",
     "iopub.status.busy": "2021-05-11T17:22:27.159712Z",
     "iopub.status.idle": "2021-05-11T17:22:59.944465Z",
     "shell.execute_reply": "2021-05-11T17:22:59.944894Z"
    },
    "papermill": {
     "duration": 32.808727,
     "end_time": "2021-05-11T17:22:59.945048",
     "exception": false,
     "start_time": "2021-05-11T17:22:27.136321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/shopee-libs/imagesize-1.2.0-py2.py3-none-any.whl\n",
      "Processing /kaggle/input/shopee-libs/PyStemmer-2.0.1/dist/PyStemmer-2.0.1.tar\n",
      "Building wheels for collected packages: PyStemmer\n",
      "  Building wheel for PyStemmer (setup.py): started\n",
      "  Building wheel for PyStemmer (setup.py): finished with status 'done'\n",
      "  Created wheel for PyStemmer: filename=PyStemmer-2.0.1-cp37-cp37m-linux_x86_64.whl size=438157 sha256=539e244d53f9e8d76aa3cad16a0ca7cd4f5cbd98168a2f08bb66991804beea26\n",
      "  Stored in directory: /root/.cache/pip/wheels/11/0a/7f/e79252382ce2d7b722cff0d364d87fff1fcf55de1b5ef18d12\n",
      "Successfully built PyStemmer\n",
      "Installing collected packages: PyStemmer, imagesize\n",
      "Successfully installed PyStemmer-2.0.1 imagesize-1.2.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install ../input/shopee-libs/imagesize-1.2.0-py2.py3-none-any.whl \\\n",
    "../input/shopee-libs/PyStemmer-2.0.1/dist/PyStemmer-2.0.1.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:22:59.994355Z",
     "iopub.status.busy": "2021-05-11T17:22:59.993659Z",
     "iopub.status.idle": "2021-05-11T17:25:07.333346Z",
     "shell.execute_reply": "2021-05-11T17:25:07.332369Z"
    },
    "papermill": {
     "duration": 127.368879,
     "end_time": "2021-05-11T17:25:07.333575",
     "exception": false,
     "start_time": "2021-05-11T17:22:59.964696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!cp ../input/d/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:25:07.441354Z",
     "iopub.status.busy": "2021-05-11T17:25:07.435899Z",
     "iopub.status.idle": "2021-05-11T17:26:16.115876Z",
     "shell.execute_reply": "2021-05-11T17:26:16.115214Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 68.734667,
     "end_time": "2021-05-11T17:26:16.116039",
     "exception": false,
     "start_time": "2021-05-11T17:25:07.381372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudf/types.hpp(27): warning: thrust/optional.h: [jitify] File not found\n",
      "cudf/types.hpp(29): warning: cassert: [jitify] File not found\n",
      "cudf/utilities/bit.hpp(19): warning: cassert: [jitify] File not found\n",
      "cudf/fixed_point/fixed_point.hpp(27): warning: cassert: [jitify] File not found\n",
      "  get image size - done in 0.02374s\n",
      "Computing text embeddings...\n",
      "text embeddings shape (1000, 15419)\n",
      "  tfidf fit - done in 13.86734s\n",
      "Finding similar titles...\n",
      "chunk 0 to 1000 1000\n",
      "  tfidf pred - done in 3.96267s\n",
      "  add edges - done in 0.05840s\n",
      "  pagerank - done in 0.01788s\n",
      "  pagerank get - done in 0.00156s\n",
      "  add edges - done in 0.05675s\n",
      "  pagerank - done in 0.01565s\n",
      "  pagerank get - done in 0.00264s\n",
      "  add edges - done in 0.05602s\n",
      "  pagerank - done in 0.01555s\n",
      "  pagerank get - done in 0.00243s\n",
      "  add edges - done in 0.03259s\n",
      "  pagerank - done in 0.00920s\n",
      "  pagerank get - done in 0.00292s\n",
      "  add edges - done in 0.05901s\n",
      "  pagerank - done in 0.01605s\n",
      "  pagerank get - done in 0.00258s\n",
      "[17:26:13] ../src/frontend/lightgbm.cc:544: model.num_tree = 3878\n",
      "[1.0]\n",
      "[0.345]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:13<00:00, 75.69it/s]\n",
      "graph: 100%|██████████| 5/5 [00:00<00:00,  6.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 4201.87it/s]\n",
      "pred chunk: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import ast\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import langid\n",
    "import Levenshtein\n",
    "\n",
    "#import albumentations\n",
    "#from albumentations import *\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "map_used_time = defaultdict(float)\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    tt = time.time() - t0\n",
    "    map_used_time[title] += tt\n",
    "    print(\"  {} - done in {:.5f}s\".format(title, tt))\n",
    "\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "import imagesize\n",
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('indonesian')\n",
    "DEBUG = len(pd.read_csv('../input/shopee-product-matching/test.csv')) == 3\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    data_dir = '../input/shopee-product-matching/train_images/'\n",
    "else:\n",
    "    data_dir = '../input/shopee-product-matching/test_images/'\n",
    "    \n",
    "###\n",
    "\n",
    "if DEBUG:\n",
    "    if 1:\n",
    "        nrows = 1000\n",
    "        df_test = pd.read_csv('../input/shopee-libs/train_newfold_stmmedid.csv', nrows=nrows)\n",
    "    else:\n",
    "        df_test = pd.read_csv('../input/shopee-libs/train_newfold_stmmedid.csv').append(\n",
    "            pd.read_csv('../input/shopee-libs/train_newfold_stmmedid.csv'), ignore_index=True\n",
    "        )\n",
    "    \n",
    "    label_groups = np.sort(df_test['label_group'].unique())\n",
    "    map_label2id = {g: i for i, g in enumerate(label_groups)}\n",
    "    df_test['label'] = df_test['label_group'].map(map_label2id)\n",
    "    df_test['file_path'] = df_test.image.apply(lambda x: os.path.join(data_dir, f'{x}'))\n",
    "else:\n",
    "    df_test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "    df_test['file_path'] = df_test.image.apply(lambda x: os.path.join(data_dir, f'{x}'))\n",
    "\n",
    "    titles = df_test['title'].str.lower().values\n",
    "\n",
    "    with timer('get lang'):\n",
    "        df_test['lang'] = [langid.classify(t)[0] for t in tqdm(titles)]\n",
    "        list_lang = df_test['lang'].values\n",
    "    with timer('lemmatize'):\n",
    "        titles = np.array([t.encode('ascii').decode('unicode-escape').encode('ascii', 'replace').decode('ascii').replace('?', ' ') for t in titles])\n",
    "        titles = [' '.join(stemmer.stemWords(t.split())) if list_lang[i] in {'id', 'ms'} else t for i, t in enumerate(tqdm(titles))]\n",
    "        df_test['title'] = titles\n",
    "\n",
    "with timer('get image size'):\n",
    "    st_sizes, img_hs, img_ws = joblib.load('/tmp/lyk_img_meta_data.pkl')\n",
    "    df_test['width'] = img_ws\n",
    "    df_test['hight'] = img_hs\n",
    "    df_test['st_size'] = st_sizes\n",
    "    df_test['wxh/st_size'] = df_test['width'] * df_test['hight'] / df_test['st_size']\n",
    "\n",
    "df_test.to_pickle('/tmp/df_test_tkm.pkl')\n",
    "###\n",
    "\n",
    "K = min(60, df_test.shape[0])\n",
    "\n",
    "###\n",
    "print('Computing text embeddings...')\n",
    "import cupy as cp\n",
    "import pickle\n",
    "import gc\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "import cudf\n",
    "\n",
    "model = TfidfVectorizer(stop_words=None, \n",
    "                        binary=True, \n",
    "                        max_features=100000,\n",
    "                        max_df=0.3,\n",
    "                        min_df=2,\n",
    "                        dtype=np.float32)\n",
    "\n",
    "with timer('tfidf fit'):\n",
    "    titles = pd.read_csv('../input/shopee-libs/train_newfold_stmmedid.csv', \n",
    "                         usecols=['title'])['title'].values.tolist()\n",
    "    test_titles = df_test.title.values.tolist()\n",
    "    titles += test_titles\n",
    "    model.fit(cudf.Series(titles))\n",
    "    text_embeddings = model.transform(cudf.Series(test_titles))\n",
    "    print('text embeddings shape',text_embeddings.shape)\n",
    "\n",
    "with timer('tfidf pred'):\n",
    "    CHUNK = 1024*4\n",
    "    print('Finding similar titles...')\n",
    "    text_D = np.zeros((df_test.shape[0], K), dtype=np.float32)\n",
    "    text_I = np.zeros((df_test.shape[0], K), dtype=np.int32)\n",
    "\n",
    "\n",
    "    CTS = text_embeddings.shape[0]//CHUNK\n",
    "    if  text_embeddings.shape[0]%CHUNK!=0: CTS += 1\n",
    "    cnt = 0\n",
    "    for j in range( CTS ):\n",
    "\n",
    "        a = j*CHUNK\n",
    "        b = (j+1)*CHUNK\n",
    "        b = min(b, text_embeddings.shape[0])\n",
    "        print('chunk',a,'to',b, text_embeddings.shape[0])\n",
    "\n",
    "        #COSINE SIMILARITY DISTANCE\n",
    "        cts = (text_embeddings * text_embeddings[a:b].T).T.toarray()\n",
    "        indices = cp.argsort(cts, axis=1)\n",
    "\n",
    "        for k in range(b-a):\n",
    "            idx = indices[k][::-1]\n",
    "            text_I[cnt] = idx[:K].get()\n",
    "            text_D[cnt] = cts[k, idx[:K]].get()\n",
    "            cnt += 1\n",
    "\n",
    "del text_embeddings, indices, cts\n",
    "gc.collect()\n",
    "###\n",
    "\n",
    "img_D = np.load('/tmp/img_D_qe.npy')\n",
    "img_I = np.load('/tmp/img_I_qe.npy')\n",
    "\n",
    "###\n",
    "\n",
    "bert_D = np.load('/tmp/brt_D_qe.npy')\n",
    "bert_I = np.load('/tmp/brt_I_qe.npy')\n",
    "\n",
    "###\n",
    "\n",
    "bth_D = np.load('/tmp/bth_D_qe.npy')\n",
    "bth_I = np.load('/tmp/bth_I_qe.npy')\n",
    "###\n",
    "\n",
    "mut_D = np.load('/tmp/mut_D_qe.npy')\n",
    "mut_I = np.load('/tmp/mut_I_qe.npy')\n",
    "###\n",
    "\n",
    "map_col2id = {}\n",
    "###\n",
    "\n",
    "import langid\n",
    "import Levenshtein\n",
    "titles = df_test['title'].values\n",
    "titles_set = [set(t) for t in titles]\n",
    "langs = df_test['lang'].values\n",
    "st_size = df_test['st_size'].values\n",
    "wh_st_size = df_test['wxh/st_size'].values\n",
    "###\n",
    "\n",
    "numset = set('0123456789')\n",
    "\n",
    "###\n",
    "text_D = np.array(text_D)\n",
    "txt_cnt_all = np.vstack([(text_D > t).sum(axis=1) for t in [0.9, 0.8, 0.7, 0.6, 0.5]]).T\n",
    "txt_avg_raw_all = text_D.mean(axis=1)\n",
    "txt_avg_all = (txt_avg_raw_all - txt_avg_raw_all.mean()) / txt_avg_raw_all.std()\n",
    "txt_std_all = text_D.std(axis=1)\n",
    "\n",
    "txt_avg_5_all = text_D[:, :5].mean(axis=1)\n",
    "txt_avg_10_all = text_D[:, :10].mean(axis=1)\n",
    "txt_avg_15_all = text_D[:, :15].mean(axis=1)\n",
    "txt_avg_30_all = text_D[:, :30].mean(axis=1)\n",
    "\n",
    "txt_avg_5_all = (txt_avg_5_all - txt_avg_5_all.mean()) / txt_avg_5_all.std()\n",
    "txt_avg_10_all = (txt_avg_10_all - txt_avg_10_all.mean()) / txt_avg_10_all.std()\n",
    "txt_avg_15_all = (txt_avg_15_all - txt_avg_15_all.mean()) / txt_avg_15_all.std()\n",
    "txt_avg_30_all = (txt_avg_30_all - txt_avg_30_all.mean()) / txt_avg_30_all.std()\n",
    "    \n",
    "###\n",
    "brt_cnt_all = np.vstack([(bert_D > t).sum(axis=1) for t in [0.9, 0.8, 0.7, 0.6, 0.5]]).T\n",
    "brt_avg_raw_all = bert_D.mean(axis=1)\n",
    "brt_avg_all = (brt_avg_raw_all - brt_avg_raw_all.mean()) / brt_avg_raw_all.std()\n",
    "brt_std_all = bert_D.std(axis=1)\n",
    "\n",
    "brt_avg_5_all = bert_D[:, :5].mean(axis=1)\n",
    "brt_avg_10_all = bert_D[:, :10].mean(axis=1)\n",
    "brt_avg_15_all = bert_D[:, :15].mean(axis=1)\n",
    "brt_avg_30_all = bert_D[:, :30].mean(axis=1)\n",
    "\n",
    "brt_avg_5_all = (brt_avg_5_all - brt_avg_5_all.mean()) / brt_avg_5_all.std()\n",
    "brt_avg_10_all = (brt_avg_10_all - brt_avg_10_all.mean()) / brt_avg_10_all.std()\n",
    "brt_avg_15_all = (brt_avg_15_all - brt_avg_15_all.mean()) / brt_avg_15_all.std()\n",
    "brt_avg_30_all = (brt_avg_30_all - brt_avg_30_all.mean()) / brt_avg_30_all.std()\n",
    "\n",
    "###\n",
    "bth_cnt_all = np.vstack([(bth_D > t).sum(axis=1) for t in [0.9, 0.8, 0.7, 0.6, 0.5]]).T\n",
    "bth_avg_raw_all = bth_D.mean(axis=1)\n",
    "bth_avg_all = (bth_avg_raw_all - bth_avg_raw_all.mean()) / bth_avg_raw_all.std()\n",
    "bth_std_all = bth_D.std(axis=1)\n",
    "\n",
    "bth_avg_5_all = bth_D[:, :5].mean(axis=1)\n",
    "bth_avg_10_all = bth_D[:, :10].mean(axis=1)\n",
    "bth_avg_15_all = bth_D[:, :15].mean(axis=1)\n",
    "bth_avg_30_all = bth_D[:, :30].mean(axis=1)\n",
    "\n",
    "bth_avg_5_all = (bth_avg_5_all - bth_avg_5_all.mean()) / bth_avg_5_all.std()\n",
    "bth_avg_10_all = (bth_avg_10_all - bth_avg_10_all.mean()) / bth_avg_10_all.std()\n",
    "bth_avg_15_all = (bth_avg_15_all - bth_avg_15_all.mean()) / bth_avg_15_all.std()\n",
    "bth_avg_30_all = (bth_avg_30_all - bth_avg_30_all.mean()) / bth_avg_30_all.std()\n",
    "        \n",
    "###\n",
    "mut_cnt_all = np.vstack([(mut_D > t).sum(axis=1) for t in [0.9, 0.8, 0.7, 0.6, 0.5]]).T\n",
    "mut_avg_raw_all = mut_D.mean(axis=1)\n",
    "mut_avg_all = (mut_avg_raw_all - mut_avg_raw_all.mean()) / mut_avg_raw_all.std()\n",
    "mut_std_all = mut_D.std(axis=1)\n",
    "\n",
    "mut_avg_5_all = mut_D[:, :5].mean(axis=1)\n",
    "mut_avg_10_all = mut_D[:, :10].mean(axis=1)\n",
    "mut_avg_15_all = mut_D[:, :15].mean(axis=1)\n",
    "mut_avg_30_all = mut_D[:, :30].mean(axis=1)\n",
    "\n",
    "mut_avg_5_all = (mut_avg_5_all - mut_avg_5_all.mean()) / mut_avg_5_all.std()\n",
    "mut_avg_10_all = (mut_avg_10_all - mut_avg_10_all.mean()) / mut_avg_10_all.std()\n",
    "mut_avg_15_all = (mut_avg_15_all - mut_avg_15_all.mean()) / mut_avg_15_all.std()\n",
    "mut_avg_30_all = (mut_avg_30_all - mut_avg_30_all.mean()) / mut_avg_30_all.std()\n",
    "        \n",
    "###\n",
    "img_cnt_all = np.vstack([(img_D > t).sum(axis=1) for t in [0.9, 0.8, 0.7, 0.6, 0.5]]).T\n",
    "img_avg_raw_all = img_D.mean(axis=1)\n",
    "img_avg_all = (img_avg_raw_all - img_avg_raw_all.mean()) / img_avg_raw_all.std()\n",
    "img_std_all = img_D.std(axis=1)\n",
    "\n",
    "img_avg_5_all = img_D[:, :5].mean(axis=1)\n",
    "img_avg_10_all = img_D[:, :10].mean(axis=1)\n",
    "img_avg_15_all = img_D[:, :15].mean(axis=1)\n",
    "img_avg_30_all = img_D[:, :30].mean(axis=1)\n",
    "\n",
    "img_avg_5_all = (img_avg_5_all - img_avg_5_all.mean()) / img_avg_5_all.std()\n",
    "img_avg_10_all = (img_avg_10_all - img_avg_10_all.mean()) / img_avg_10_all.std()\n",
    "img_avg_15_all = (img_avg_15_all - img_avg_15_all.mean()) / img_avg_15_all.std()\n",
    "img_avg_30_all = (img_avg_30_all - img_avg_30_all.mean()) / img_avg_30_all.std()\n",
    "\n",
    "width_hight = df_test[['width', 'hight']].values\n",
    "\n",
    "list_pred_id = [[] for _ in range(df_test.shape[0])]\n",
    "\n",
    "indices = df_test.index.values\n",
    "\n",
    "ptr = 0\n",
    "all_feat = np.memmap('/tmp/tkm_feat.dat', dtype='float32', mode='w+', shape=(df_test.shape[0] * 60 * 5, 150), order='F')\n",
    "\n",
    "feat = np.zeros((60 * 5, 150), dtype='float32')\n",
    "\n",
    "list_idx = []\n",
    "list_idx2 = []\n",
    "list_feats = []\n",
    "for i in tqdm(indices):\n",
    "    img_d = img_D[i]\n",
    "    img_i = img_I[i]\n",
    "\n",
    "    img_cnt = img_cnt_all[i]\n",
    "    img_avg = img_avg_all[i]\n",
    "    img_std = img_std_all[i]\n",
    "\n",
    "    img_width ,img_hight = width_hight[i]\n",
    "\n",
    "    ###\n",
    "    txt_d = text_D[i]\n",
    "    txt_i = text_I[i]\n",
    "\n",
    "    txt_cnt = txt_cnt_all[i]\n",
    "    txt_avg = txt_avg_all[i]\n",
    "    txt_std = txt_std_all[i]\n",
    "\n",
    "    txt_set = set(titles[i])\n",
    "    ###\n",
    "    brt_d = bert_D[i]\n",
    "    brt_i = bert_I[i]\n",
    "\n",
    "    brt_cnt = brt_cnt_all[i]\n",
    "    brt_avg = brt_avg_all[i]\n",
    "    brt_std = brt_std_all[i]\n",
    "\n",
    "    brt_set = set(titles[i])\n",
    "    bth_d = bth_D[i]\n",
    "    bth_i = bth_I[i]\n",
    "\n",
    "    bth_cnt = bth_cnt_all[i]\n",
    "    bth_avg = bth_avg_all[i]\n",
    "    bth_std = bth_std_all[i]\n",
    "\n",
    "    bth_set = set(titles[i])\n",
    "    mut_d = mut_D[i]\n",
    "    mut_i = mut_I[i]\n",
    "\n",
    "    mut_cnt = mut_cnt_all[i]\n",
    "    mut_avg = mut_avg_all[i]\n",
    "    mut_std = mut_std_all[i]\n",
    "\n",
    "    mut_set = set(titles[i])\n",
    "\n",
    "    map_feat = {}\n",
    "    for j in range(K):\n",
    "        _w, _h = width_hight[img_i[j]]\n",
    "        _img_cnt = img_cnt_all[img_i[j]]\n",
    "        _img_avg = img_avg_all[img_i[j]]\n",
    "        _img_std = img_std_all[img_i[j]]\n",
    "\n",
    "        diff_width = abs(img_width - _w)\n",
    "        diff_hight = abs(img_hight - _h)\n",
    "        d = {\n",
    "            'img_sim': img_d[j],\n",
    "            'img_avg': img_avg, \n",
    "            'img_std': img_std,\n",
    "            'img_avg2': _img_avg, \n",
    "            'img_std2': _img_std,\n",
    "\n",
    "            'img_avg_raw': img_avg_raw_all[i],\n",
    "            'img_avg2_raw': img_avg_raw_all[img_i[j]],\n",
    "\n",
    "            'diff_width': diff_width,\n",
    "            'diff_hight': diff_hight,\n",
    "            'img_width': img_width,\n",
    "            'img_hight': img_hight,\n",
    "            'img_width2': _w,\n",
    "            'img_hight2': _h,\n",
    "\n",
    "            'st_size': st_size[i],\n",
    "            'st_size2': st_size[img_i[j]],\n",
    "            'wh_st_size': wh_st_size[i],\n",
    "            'wh_st_size2': wh_st_size[img_i[j]]\n",
    "        }\n",
    "        d.update({f'img_cnt_{ii}': img_cnt[ii] for ii in range(img_cnt.shape[0])})\n",
    "        d.update({f'img_cnt2_{ii}': _img_cnt[ii] for ii in range(_img_cnt.shape[0])})\n",
    "        map_feat[img_i[j]] = d\n",
    "        \n",
    "    for j in range(K):\n",
    "        _txt_set = titles_set[txt_i[j]]\n",
    "        _txt_cnt = txt_cnt_all[txt_i[j]]\n",
    "        _txt_avg = txt_avg_all[txt_i[j]]\n",
    "        _txt_std = txt_std_all[txt_i[j]]\n",
    "        diff_txt_set = set(titles[txt_i[j]]) & txt_set\n",
    "        diff_txt_set = len(numset & diff_txt_set) / (len(diff_txt_set) + 1)\n",
    "        xor_txt_set = set(titles[txt_i[j]]) ^ txt_set\n",
    "        xor_txt_set = len(numset & xor_txt_set) / (len(xor_txt_set) + 1)\n",
    "        jac_txt = len(txt_set & _txt_set) / (len(txt_set | _txt_set) + 1)\n",
    "        lev_dist = Levenshtein.distance(titles[i], titles[txt_i[j]])\n",
    "        d = {\n",
    "            'txt_sim': txt_d[j],\n",
    "            'txt_avg': txt_avg, \n",
    "            'txt_std': txt_std,\n",
    "            'txt_avg2': _txt_avg,\n",
    "            'txt_std2': _txt_std,\n",
    "\n",
    "            'txt_avg_raw': txt_avg_raw_all[i],\n",
    "            'txt_avg2_raw': txt_avg_raw_all[txt_i[j]],\n",
    "\n",
    "            'jac_txt': jac_txt,\n",
    "            'diff_txt_set': diff_txt_set, \n",
    "            'xor_txt_set': xor_txt_set,\n",
    "            'lev_dist': lev_dist,\n",
    "            'len_txt': len(titles[i]), \n",
    "            'len_txt2': len(titles[txt_i[j]]),\n",
    "            'lang_en': int(langs[i] == 'en'),\n",
    "            'lang_en2': int(langs[txt_i[j]] == 'en'),\n",
    "        }\n",
    "        d.update({f'txt_cnt_{ii}': txt_cnt[ii] for ii in range(txt_cnt.shape[0])})\n",
    "        d.update({f'txt_cnt2_{ii}': _txt_cnt[ii] for ii in range(_txt_cnt.shape[0])})\n",
    "        if txt_i[j] in map_feat:\n",
    "            map_feat[txt_i[j]].update(d)\n",
    "        else:\n",
    "            map_feat[txt_i[j]] = d\n",
    "            \n",
    "    for j in range(K):\n",
    "        _bth_cnt = bth_cnt_all[bth_i[j]]\n",
    "        _bth_avg = bth_avg_all[bth_i[j]]\n",
    "        _bth_std = bth_std_all[bth_i[j]]\n",
    "        if bth_i[j] in map_feat:\n",
    "            d = map_feat[bth_i[j]]\n",
    "        else:\n",
    "            d = {}\n",
    "        d.update({\n",
    "            'bth_sim': bth_d[j],\n",
    "            'bth_avg': bth_avg, \n",
    "            'bth_std': bth_std,\n",
    "            'bth_avg2': _bth_avg,\n",
    "            'bth_std2': _bth_std,\n",
    "\n",
    "            'bth_avg_raw': bth_avg_raw_all[i],\n",
    "            'bth_avg2_raw': bth_avg_raw_all[bth_i[j]],\n",
    "        })\n",
    "        d.update({f'bth_cnt_{ii}': bth_cnt[ii] for ii in range(bth_cnt.shape[0])})\n",
    "        d.update({f'bth_cnt2_{ii}': _bth_cnt[ii] for ii in range(_bth_cnt.shape[0])})\n",
    "        if 'lev_dist' not in d:\n",
    "            _bth_set = titles_set[bth_i[j]] #set(titles[bth_i[j]])\n",
    "            diff_bth_set = set(titles[bth_i[j]]) & bth_set\n",
    "            diff_bth_set = len(numset & diff_bth_set) / (len(diff_bth_set) + 1)\n",
    "            xor_bth_set = set(titles[bth_i[j]]) ^ bth_set\n",
    "            xor_bth_set = len(numset & xor_bth_set) / (len(xor_bth_set) + 1)\n",
    "            jac_bth = len(bth_set & _bth_set) / (len(bth_set | _bth_set) + 1)\n",
    "            lev_dist = Levenshtein.distance(titles[i], titles[bth_i[j]])\n",
    "            d.update({\n",
    "                'jac_txt': jac_bth,\n",
    "                'diff_txt_set': diff_bth_set, \n",
    "                'xor_txt_set': xor_bth_set,\n",
    "                'lev_dist': lev_dist,\n",
    "                'len_txt': len(titles[i]), \n",
    "                'len_txt2': len(titles[bth_i[j]]),\n",
    "                'lang_en': int(langs[i] == 'en'),\n",
    "                'lang_en2': int(langs[bth_i[j]] == 'en'),\n",
    "            })\n",
    "        if 'img_width' not in d:    \n",
    "            _w, _h = width_hight[bth_i[j]]\n",
    "            diff_width = abs(img_width - _w)\n",
    "            diff_hight = abs(img_hight - _h)\n",
    "            d.update({\n",
    "                'diff_width': diff_width,\n",
    "                'diff_hight': diff_hight,\n",
    "                 'img_width': img_width,\n",
    "                 'img_hight': img_hight,\n",
    "                 'img_width2': _w,\n",
    "                 'img_hight2': _h,\n",
    "                \n",
    "                     'st_size': st_size[i],\n",
    "                     'st_size2': st_size[bth_i[j]],\n",
    "                     'wh_st_size': wh_st_size[i],\n",
    "                     'wh_st_size2': wh_st_size[bth_i[j]]\n",
    "                     })\n",
    "        map_feat[bth_i[j]] = d\n",
    "            \n",
    "    for j in range(K):\n",
    "        _mut_cnt = mut_cnt_all[mut_i[j]]\n",
    "        _mut_avg = mut_avg_all[mut_i[j]]\n",
    "        _mut_std = mut_std_all[mut_i[j]]\n",
    "        if mut_i[j] in map_feat:\n",
    "            d = map_feat[mut_i[j]]\n",
    "        else:\n",
    "            d = {}\n",
    "        d.update({\n",
    "            'mut_sim': mut_d[j],\n",
    "            'mut_avg': mut_avg, \n",
    "            'mut_std': mut_std,\n",
    "            'mut_avg2': _mut_avg,\n",
    "            'mut_std2': _mut_std,\n",
    "            'mut_avg_raw': mut_avg_raw_all[i],\n",
    "            'mut_avg2_raw': mut_avg_raw_all[mut_i[j]],\n",
    "        })\n",
    "        d.update({f'mut_cnt_{ii}': mut_cnt[ii] for ii in range(mut_cnt.shape[0])})\n",
    "        d.update({f'mut_cnt2_{ii}': _mut_cnt[ii] for ii in range(_mut_cnt.shape[0])})\n",
    "        if 'lev_dist' not in d:\n",
    "            _mut_set = titles_set[mut_i[j]]#set(titles[mut_i[j]])\n",
    "            diff_mut_set = set(titles[mut_i[j]]) & mut_set\n",
    "            diff_mut_set = len(numset & diff_mut_set) / (len(diff_mut_set) + 1)\n",
    "            xor_mut_set = set(titles[mut_i[j]]) ^ mut_set\n",
    "            xor_mut_set = len(numset & xor_mut_set) / (len(xor_mut_set) + 1)\n",
    "            jac_mut = len(mut_set & _mut_set) / (len(mut_set | _mut_set) + 1)\n",
    "            lev_dist = Levenshtein.distance(titles[i], titles[mut_i[j]])\n",
    "            d.update({\n",
    "                'jac_txt': jac_mut,\n",
    "                'diff_txt_set': diff_mut_set, \n",
    "                'xor_txt_set': xor_mut_set,\n",
    "                'lev_dist': lev_dist,\n",
    "                'len_txt': len(titles[i]), \n",
    "                'len_txt2': len(titles[mut_i[j]]),\n",
    "                'lang_en': int(langs[i] == 'en'),\n",
    "                'lang_en2': int(langs[mut_i[j]] == 'en'),\n",
    "            })\n",
    "        if 'img_width' not in d:    \n",
    "            _w, _h = width_hight[mut_i[j]]\n",
    "            diff_width = abs(img_width - _w)\n",
    "            diff_hight = abs(img_hight - _h)\n",
    "            d.update({\n",
    "                'diff_width': diff_width,\n",
    "                'diff_hight': diff_hight,\n",
    "                'img_width': img_width,\n",
    "                'img_hight': img_hight,\n",
    "                'img_width2': _w,\n",
    "                'img_hight2': _h,\n",
    "                'st_size': st_size[i],\n",
    "                'st_size2': st_size[mut_i[j]],\n",
    "                'wh_st_size': wh_st_size[i],\n",
    "                'wh_st_size2': wh_st_size[mut_i[j]]\n",
    "            })\n",
    "        map_feat[mut_i[j]] = d\n",
    "\n",
    "    for j in range(K):\n",
    "        _brt_cnt = brt_cnt_all[brt_i[j]]\n",
    "        _brt_avg = brt_avg_all[brt_i[j]]\n",
    "        _brt_std = brt_std_all[brt_i[j]]\n",
    "        if brt_i[j] in map_feat:\n",
    "            d = map_feat[brt_i[j]]\n",
    "        else:\n",
    "            d = {}\n",
    "        d.update({\n",
    "            'brt_sim': brt_d[j],\n",
    "            'brt_avg': brt_avg, \n",
    "            'brt_std': brt_std,\n",
    "            'brt_avg2': _brt_avg,\n",
    "            'brt_std2': _brt_std,\n",
    "            'brt_avg_raw': brt_avg_raw_all[i],\n",
    "            'brt_avg2_raw': brt_avg_raw_all[brt_i[j]],\n",
    "        })\n",
    "        d.update({f'brt_cnt_{ii}': brt_cnt[ii] for ii in range(brt_cnt.shape[0])})\n",
    "        d.update({f'brt_cnt2_{ii}': _brt_cnt[ii] for ii in range(_brt_cnt.shape[0])})\n",
    "        if 'lev_dist' not in d:\n",
    "            _brt_set = titles_set[brt_i[j]]\n",
    "            diff_brt_set = set(titles[brt_i[j]]) & brt_set\n",
    "            diff_brt_set = len(numset & diff_brt_set) / (len(diff_brt_set) + 1)\n",
    "            xor_brt_set = set(titles[brt_i[j]]) ^ brt_set\n",
    "            xor_brt_set = len(numset & xor_brt_set) / (len(xor_brt_set) + 1)\n",
    "            jac_brt = len(brt_set & _brt_set) / (len(brt_set | _brt_set) + 1)\n",
    "            lev_dist = Levenshtein.distance(titles[i], titles[brt_i[j]])\n",
    "            d.update({\n",
    "                'jac_txt': jac_brt,\n",
    "                'diff_txt_set': diff_brt_set, \n",
    "                'xor_txt_set': xor_brt_set,\n",
    "                'lev_dist': lev_dist,\n",
    "                'len_txt': len(titles[i]), \n",
    "                'len_txt2': len(titles[brt_i[j]]),\n",
    "                'lang_en': int(langs[i] == 'en'),\n",
    "                'lang_en2': int(langs[brt_i[j]] == 'en'),\n",
    "            })\n",
    "        map_feat[brt_i[j]] = d\n",
    "\n",
    "    feat[:] = 0 \n",
    "    for ii, (k, map_val) in enumerate(map_feat.items()):\n",
    "        list_idx.append(i)\n",
    "        list_idx2.append(k)\n",
    "        for c, v in map_val.items():\n",
    "            if c not in map_col2id:\n",
    "                map_col2id[c] = len(map_col2id)\n",
    "            feat[ii, map_col2id[c]] = v\n",
    "\n",
    "    all_feat[ptr:ptr + len(map_feat)] = feat[:len(map_feat)]\n",
    "    ptr += len(map_feat)\n",
    "    \n",
    "del img_D, img_I, text_D, text_I, bert_D, bert_I, bth_D, bth_I, mut_D, mut_I\n",
    "gc.collect()\n",
    "\n",
    "del list_feats\n",
    "gc.collect()\n",
    "\n",
    "map_weights = {sim: all_feat[:ptr, map_col2id[f'{sim}_sim']] for sim in ['img', 'bth', 'mut', 'txt', 'brt']}\n",
    "\n",
    "del feat\n",
    "gc.collect()\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "list_idx = np.array(list_idx)\n",
    "list_idx2 = np.array(list_idx2)\n",
    "\n",
    "from igraph import Graph\n",
    "map_sim = {}\n",
    "for sim in tqdm(['img', 'bth', 'mut', 'txt', 'brt'], desc='graph'):\n",
    "    weights = map_weights[sim]\n",
    "    idx = weights > 0\n",
    "    with timer('add edges'):\n",
    "        g = Graph()\n",
    "        g.add_vertices(len(df_test))\n",
    "        g.add_edges(list(zip(list_idx[idx], list_idx2[idx])), {'weight': weights[idx]})\n",
    "    with timer('pagerank'):\n",
    "        map_pr = np.array(g.pagerank(damping=0.85, weights='weight', niter=100, eps=1e-06, directed=False))\n",
    "    with timer('pagerank get'):\n",
    "        data1 = map_pr[list_idx]\n",
    "        data2 = map_pr[list_idx2]\n",
    "        data1[weights <= 0] = 0\n",
    "        data2[weights <= 0] = 0\n",
    "        map_sim[f'{sim}_pagerank'] = data1\n",
    "        map_sim[f'{sim}_pagerank2'] = data2\n",
    "    del map_pr, g\n",
    "    gc.collect()\n",
    "\n",
    "for c, v in tqdm(map_sim.items()):\n",
    "    map_col2id[c] = len(map_col2id)\n",
    "    all_feat[:ptr, map_col2id[c]] = v\n",
    "\n",
    "import treelite_runtime\n",
    "from cuml import ForestInference\n",
    "import treelite\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "all_weights = {\n",
    "    '../input/shopee-metric-resnet50d512-0328-newfold/0508_qe_best_0.345/': 1,\n",
    "}\n",
    "\n",
    "s = sum(all_weights.values())\n",
    "all_weights = {k: v / s for k, v in all_weights.items()}\n",
    "    \n",
    "list_clf = []\n",
    "weights = []\n",
    "thresholds = [] #[0.358, 0.361, 0.350, 0.336, 0.348, 0.346]\n",
    "for path in [\n",
    "    '../input/shopee-metric-resnet50d512-0328-newfold/0508_qe_best_0.345/',\n",
    "    ]:\n",
    "    name = os.path.dirname(path).split('/')[-1]\n",
    "    th = float(name.split('_')[-1])\n",
    "    if all_weights.get(path, 0) == 0:\n",
    "        continue\n",
    "        \n",
    "    fi = ForestInference()\n",
    "    fi.load_from_treelite_model(treelite.Model.load(f'{path}/all_data_clf_norm.lgb', model_format='lightgbm'))\n",
    "    list_clf += [fi]\n",
    "    thresholds += [th]\n",
    "    weights += [all_weights[path]]\n",
    "    \n",
    "print(weights)\n",
    "print(thresholds)\n",
    "\n",
    "col = lgb.Booster(model_file=f'{path}/all_data_clf_norm.lgb').feature_name()\n",
    "\n",
    "for sf in ['img', 'txt', 'mut', 'bth', 'brt']:\n",
    "    all_feat[:ptr, map_col2id[f'{sf}_avg']] = all_feat[:ptr, map_col2id[f'{sf}_avg_raw']]\n",
    "    all_feat[:ptr, map_col2id[f'{sf}_avg2']] = all_feat[:ptr, map_col2id[f'{sf}_avg2_raw']]\n",
    "\n",
    "CHUNK = 1000000\n",
    "preds = []\n",
    "col_idx = [map_col2id[c] for c in col]\n",
    "\n",
    "for ch in tqdm(range(0, ptr, CHUNK), desc='pred chunk'):\n",
    "    feat = cp.asarray(all_feat[ch:ch+CHUNK, col_idx]).astype('float32')\n",
    "    probs = np.vstack([(c.predict(feat).get() - thresholds[ii]) * weights[ii] for ii, c in enumerate(list_clf)])\n",
    "    preds += probs.sum(axis=0).tolist()\n",
    "    del feat\n",
    "    gc.collect()\n",
    "\n",
    "df_pred = pd.DataFrame(\n",
    "    dict(\n",
    "        posting_id=list_idx,\n",
    "        posting_id_target=list_idx2,\n",
    "        pred=preds[:ptr]\n",
    "    )\n",
    ")\n",
    "\n",
    "idx = df_test.posting_id.values\n",
    "df_pred['posting_id'] = [idx[i] for i in df_pred['posting_id'].values]\n",
    "df_pred['posting_id_target'] = [idx[i] for i in df_pred['posting_id_target'].values]\n",
    "\n",
    "df_pred.to_pickle('submission_tkm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020378,
     "end_time": "2021-05-11T17:26:16.159450",
     "exception": false,
     "start_time": "2021-05-11T17:26:16.139072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:26:16.206222Z",
     "iopub.status.busy": "2021-05-11T17:26:16.205451Z",
     "iopub.status.idle": "2021-05-11T17:26:17.022203Z",
     "shell.execute_reply": "2021-05-11T17:26:17.021157Z"
    },
    "papermill": {
     "duration": 0.842369,
     "end_time": "2021-05-11T17:26:17.022348",
     "exception": false,
     "start_time": "2021-05-11T17:26:16.179979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_lyk = pd.read_pickle('submission_lyak.pkl')\n",
    "df_lyk_gcn = pd.read_pickle('submission_lyak_gcn.pkl')\n",
    "df_tkm = pd.read_pickle('submission_tkm.pkl')\n",
    "\n",
    "df_lyk['pred'] *= 1\n",
    "df_lyk_gcn['pred'] *= 3\n",
    "df_tkm['pred'] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:26:17.092984Z",
     "iopub.status.busy": "2021-05-11T17:26:17.092117Z",
     "iopub.status.idle": "2021-05-11T17:26:17.424509Z",
     "shell.execute_reply": "2021-05-11T17:26:17.424977Z"
    },
    "papermill": {
     "duration": 0.380935,
     "end_time": "2021-05-11T17:26:17.425142",
     "exception": false,
     "start_time": "2021-05-11T17:26:17.044207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>posting_id_target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_1002628427</td>\n",
       "      <td>train_1002628427</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1010868925</td>\n",
       "      <td>train_1010868925</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_1010868925</td>\n",
       "      <td>train_4184037897</td>\n",
       "      <td>0.607435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_1011324296</td>\n",
       "      <td>train_1011324296</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_1011324296</td>\n",
       "      <td>train_1343380721</td>\n",
       "      <td>0.375346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>train_993003820</td>\n",
       "      <td>train_993003820</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>train_993079226</td>\n",
       "      <td>train_993079226</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>train_998568945</td>\n",
       "      <td>train_2413283241</td>\n",
       "      <td>0.274144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>train_998568945</td>\n",
       "      <td>train_998568945</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>train_999391364</td>\n",
       "      <td>train_999391364</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1874 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            posting_id posting_id_target      pred\n",
       "0     train_1002628427  train_1002628427  0.500000\n",
       "1     train_1010868925  train_1010868925  0.500000\n",
       "2     train_1010868925  train_4184037897  0.607435\n",
       "3     train_1011324296  train_1011324296  0.500000\n",
       "4     train_1011324296  train_1343380721  0.375346\n",
       "...                ...               ...       ...\n",
       "1869   train_993003820   train_993003820  0.500000\n",
       "1870   train_993079226   train_993079226  0.500000\n",
       "1871   train_998568945  train_2413283241  0.274144\n",
       "1872   train_998568945   train_998568945  0.500000\n",
       "1873   train_999391364   train_999391364  0.500000\n",
       "\n",
       "[1874 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.concat([df_lyk, df_lyk_gcn, df_tkm], axis=0, ignore_index=True).groupby(['posting_id', 'posting_id_target'])[['pred']].sum() / 6\n",
    "\n",
    "df_pred.reset_index(inplace=True)\n",
    "df_pred.loc[df_pred['posting_id'] == df_pred['posting_id_target'], 'pred'] = 0.5\n",
    "df_pred.set_index(['posting_id', 'posting_id_target'], inplace=True)\n",
    "\n",
    "df_pred = df_pred.query('pred > 0')\n",
    "df_pred = df_pred[df_pred.apply(lambda row: (row.name[1], row.name[0]) in df_pred.index, axis=1)].reset_index()\n",
    "\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-11T17:26:17.483846Z",
     "iopub.status.busy": "2021-05-11T17:26:17.483041Z",
     "iopub.status.idle": "2021-05-11T17:26:28.865811Z",
     "shell.execute_reply": "2021-05-11T17:26:28.865365Z"
    },
    "papermill": {
     "duration": 11.41777,
     "end_time": "2021-05-11T17:26:28.865934",
     "exception": false,
     "start_time": "2021-05-11T17:26:17.448164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_1002628427</td>\n",
       "      <td>train_1002628427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1010868925</td>\n",
       "      <td>train_4184037897 train_1010868925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_1011324296</td>\n",
       "      <td>train_1343380721 train_1011324296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_1012209986</td>\n",
       "      <td>train_1012209986 train_1969570411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_1014754278</td>\n",
       "      <td>train_1014754278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>train_990204630</td>\n",
       "      <td>train_1724760722 train_990204630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>train_993003820</td>\n",
       "      <td>train_993003820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>train_993079226</td>\n",
       "      <td>train_993079226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>train_998568945</td>\n",
       "      <td>train_998568945 train_2413283241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>train_999391364</td>\n",
       "      <td>train_999391364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           posting_id                            matches\n",
       "0    train_1002628427                   train_1002628427\n",
       "1    train_1010868925  train_4184037897 train_1010868925\n",
       "2    train_1011324296  train_1343380721 train_1011324296\n",
       "3    train_1012209986  train_1012209986 train_1969570411\n",
       "4    train_1014754278                   train_1014754278\n",
       "..                ...                                ...\n",
       "995   train_990204630   train_1724760722 train_990204630\n",
       "996   train_993003820                    train_993003820\n",
       "997   train_993079226                    train_993079226\n",
       "998   train_998568945   train_998568945 train_2413283241\n",
       "999   train_999391364                    train_999391364\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from cugraph.centrality.betweenness_centrality import edge_betweenness_centrality\n",
    "\n",
    "G = nx.Graph()\n",
    "for i, j, w in df_pred[['posting_id', 'posting_id_target', 'pred']].values:\n",
    "    G.add_edge(i, j, weight=w)\n",
    "\n",
    "list_remove_edges = []\n",
    "list_add_edges = []\n",
    "def split_graph(G):\n",
    "    list_comp = list(nx.connected_components(G))\n",
    "    n = len(G.nodes)\n",
    "    if len(list_comp) == 1:\n",
    "        map_bet = edge_betweenness_centrality(G, normalized=True)\n",
    "        map_bet = {(i, j): w  for (i, j), w in map_bet.items() \n",
    "                   if G[i][j]['weight'] < 0.15780210284453428}\n",
    "        if len(map_bet) == 0:\n",
    "            return\n",
    "        edge, val = max(map_bet.items(), key=lambda x: x[1])\n",
    "        if val > 0.11766651703447985:\n",
    "            G.remove_edge(*edge)\n",
    "            list_remove_edges.append(edge)\n",
    "            return split_graph(G)\n",
    "    else:\n",
    "        iters = list_comp\n",
    "        for comp in iters:\n",
    "            if len(comp) > 6:\n",
    "                split_graph(nx.Graph(G.subgraph(comp)))\n",
    "                \n",
    "split_graph(G)\n",
    "for edge in list_remove_edges:\n",
    "    G.remove_edge(*edge)\n",
    "\n",
    "def get_score(i, j):\n",
    "    try:\n",
    "        return G[i][j]['weight']\n",
    "    except KeyError:\n",
    "        return -1\n",
    "\n",
    "posting_ids = df_pred['posting_id'].unique()\n",
    "matches = []\n",
    "\n",
    "for i in posting_ids:\n",
    "    if i in G:\n",
    "        m = list(set([i] + list(G.neighbors(i))))\n",
    "    else:\n",
    "        m = [i]\n",
    "    if len(m) > 51:\n",
    "        m = sorted(m, key=lambda x: get_score(i, x), reverse=True)[:51]\n",
    "    matches.append(' '.join(m))\n",
    "matched = pd.DataFrame(dict(posting_id=posting_ids, matches=matches))\n",
    "\n",
    "matched.to_csv('submission.csv', index=False)\n",
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.022108,
     "end_time": "2021-05-11T17:26:28.910985",
     "exception": false,
     "start_time": "2021-05-11T17:26:28.888877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 516.70538,
   "end_time": "2021-05-11T17:26:31.281494",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-11T17:17:54.576114",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
